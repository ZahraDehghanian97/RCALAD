{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/zahraDehghanian97/Adversarially-Learned-Anomaly-Detection/blob/master/alad/ALAD_Arrythmia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4P_jI5lleO06"
   },
   "source": [
    "Mount google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_SO7XcVSP-E",
    "outputId": "91244d6f-3510-4c75-c854-b37fe53d30c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qZ4HoO4eRYy"
   },
   "source": [
    "add system path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VaHFNPl8TsPn"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pdb\n",
    "import sys\n",
    "py_file_location = \"/content/drive/MyDrive/Colab Notebooks/Adversarially-Learned-Anomaly-Detection/alad\"\n",
    "sys.path.append(os.path.abspath(py_file_location))\n",
    "py_file_location = \"/content/drive/MyDrive/Colab Notebooks/Adversarially-Learned-Anomaly-Detection/utils\"\n",
    "sys.path.append(os.path.abspath(py_file_location))\n",
    "py_file_location = \"/content/drive/MyDrive/Colab Notebooks/Adversarially-Learned-Anomaly-Detection/data\"\n",
    "sys.path.append(os.path.abspath(py_file_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaoHU_ZPIJFW"
   },
   "source": [
    "class arythmia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-dwzbfoWIMVS"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import scipy.io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class arrythmia :\n",
    "\n",
    "  def __init__(self):\n",
    "     logger = logging.getLogger(__name__)\n",
    "     return\n",
    "  \n",
    "  def get_train(self,label=0, scale=False, *args):\n",
    "      \"\"\"Get training dataset for Thyroid dataset\"\"\"\n",
    "      return self._get_adapted_dataset(\"train\", scale)\n",
    "\n",
    "  def get_test(self,label=0, scale=False, *args):\n",
    "      \"\"\"Get testing dataset for Thyroid dataset\"\"\"\n",
    "      return self._get_adapted_dataset(\"test\", scale)\n",
    "\n",
    "  def get_valid(self,label=0, scale=False, *args):\n",
    "      \"\"\"Get validation dataset for Thyroid dataset\"\"\"\n",
    "      return None\n",
    "\n",
    "  def get_shape_input(self):\n",
    "      \"\"\"Get shape of the dataset for Thyroid dataset\"\"\"\n",
    "      return (None, 274)\n",
    "\n",
    "  def get_shape_input_flatten(self):\n",
    "      \"\"\"Get shape of the dataset for Thyroid dataset\"\"\"\n",
    "      return (None, 274) \n",
    "\n",
    "  def get_shape_label(self):\n",
    "      \"\"\"Get shape of the labels in Thyroid dataset\"\"\"\n",
    "      return (None,)\n",
    "\n",
    "  def get_anomalous_proportion(self):\n",
    "      return 0.15\n",
    "\n",
    "  def _get_dataset(self,scale):\n",
    "      \"\"\" Gets the basic dataset\n",
    "      Returns :\n",
    "              dataset (dict): containing the data\n",
    "                  dataset['x_train'] (np.array): training images shape\n",
    "                  (?, 120)\n",
    "                  dataset['y_train'] (np.array): training labels shape\n",
    "                  (?,)\n",
    "                  dataset['x_test'] (np.array): testing images shape\n",
    "                  (?, 120)\n",
    "                  dataset['y_test'] (np.array): testing labels shape\n",
    "                  (?,)\n",
    "      \"\"\"\n",
    "      data = scipy.io.loadmat(\"/content/drive/MyDrive/Colab Notebooks/Adversarially-Learned-Anomaly-Detection/data/arrhythmia.mat\")\n",
    "      full_x_data = data[\"X\"]\n",
    "      full_y_data = data['y']\n",
    "      x_train, x_test, \\\n",
    "      y_train, y_test = train_test_split(full_x_data,\n",
    "                                        full_y_data,\n",
    "                                        test_size=0.5,\n",
    "                                        random_state=42)\n",
    "\n",
    "      y_train = y_train.flatten().astype(int)\n",
    "      y_test = y_test.flatten().astype(int)\n",
    "\n",
    "      if scale:\n",
    "          print(\"Scaling dataset\")\n",
    "          scaler = MinMaxScaler()\n",
    "          scaler.fit(x_train)\n",
    "          x_train = scaler.transform(x_train)\n",
    "          x_test = scaler.transform(x_test)\n",
    "\n",
    "      \n",
    "      dataset = {}\n",
    "      dataset['x_train'] = x_train.astype(np.float32)\n",
    "      dataset['y_train'] = y_train.astype(np.float32)\n",
    "      dataset['x_test'] = x_test.astype(np.float32)\n",
    "      dataset['y_test'] = y_test.astype(np.float32)\n",
    "      return dataset\n",
    "\n",
    "  def _get_adapted_dataset(self,split, scale):\n",
    "      \"\"\" Gets the adapted dataset for the experiments\n",
    "\n",
    "      Args :\n",
    "              split (str): train or test\n",
    "      Returns :\n",
    "              (tuple): <training, testing> images and labels\n",
    "      \"\"\"\n",
    "      # print(\"_get_adapted\",scale)\n",
    "      dataset = self._get_dataset(scale)\n",
    "      key_img = 'x_' + split\n",
    "      key_lbl = 'y_' + split\n",
    "      \n",
    "      print(\"Size of split\", split, \":\", dataset[key_lbl].shape[0])\n",
    "\n",
    "      return (dataset[key_img], dataset[key_lbl])\n",
    "\n",
    "  def _to_xy(self,df, target):\n",
    "      \"\"\"Converts a Pandas dataframe to the x,y inputs that TensorFlow needs\"\"\"\n",
    "      result = []\n",
    "      for x in df.columns:\n",
    "          if x != target:\n",
    "              result.append(x)\n",
    "      dummies = df[target]\n",
    "      return df.as_matrix(result).astype(np.float32), dummies.as_matrix().astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eArFwHsNeWxg"
   },
   "source": [
    "Main part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Fl5ATBlTOCJH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a1232307-b9af-4162-8c5a-e3e7922da83a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:107: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "WARNING:ALAD.run.arrhythmia.1:ALAD is training with the following parameters:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of split train : 226\n",
      "Size of split test : 226\n",
      "Batch size:  32\n",
      "Starting learning rate:  1e-05\n",
      "EMA Decay:  0.999\n",
      "Degree for L norms:  2\n",
      "Anomalous label:  1\n",
      "Score method:  \n",
      "Discriminator zz enabled:  True\n",
      "Spectral Norm enabled:  False\n",
      "WARNING:tensorflow:From /content/drive/MyDrive/Colab Notebooks/Adversarially-Learned-Anomaly-Detection/alad/arrhythmia_utilities.py:112: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /content/drive/MyDrive/Colab Notebooks/Adversarially-Learned-Anomaly-Detection/alad/arrhythmia_utilities.py:112: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /content/drive/MyDrive/Colab Notebooks/Adversarially-Learned-Anomaly-Detection/alad/arrhythmia_utilities.py:199: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /content/drive/MyDrive/Colab Notebooks/Adversarially-Learned-Anomaly-Detection/alad/arrhythmia_utilities.py:199: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /content/drive/MyDrive/Colab Notebooks/Adversarially-Learned-Anomaly-Detection/alad/arrhythmia_utilities.py:207: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /content/drive/MyDrive/Colab Notebooks/Adversarially-Learned-Anomaly-Detection/alad/arrhythmia_utilities.py:207: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-35-8a1c2a20bc3e>:290: calling norm (from tensorflow.python.ops.linalg_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-35-8a1c2a20bc3e>:290: calling norm (from tensorflow.python.ops.linalg_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-35-8a1c2a20bc3e>:360: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-35-8a1c2a20bc3e>:360: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Tensor' object has no attribute 'to_proto'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Tensor' object has no attribute 'to_proto'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting standard services.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting standard services.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoint to path train_logs/arrhythmia/alad_snFalse_dzzTrue/dzzenabledTrue//label1/rd2/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoint to path train_logs/arrhythmia/alad_snFalse_dzzTrue/dzzenabledTrue//label1/rd2/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting queue runners.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting queue runners.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Tensor' object has no attribute 'to_proto'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Tensor' object has no attribute 'to_proto'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Tensor' object has no attribute 'to_proto'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Tensor' object has no attribute 'to_proto'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | time = 2s | loss gen = 39.0156 | loss enc = 40.9102 | loss dis = 41.4140 | loss dis xz = 14.2601 | loss dis xx = 25.5672 | loss dis zz = 1.5867\n",
      "Epoch 1 | time = 0s | loss gen = 36.0839 | loss enc = 38.8532 | loss dis = 34.8959 | loss dis xz = 13.3756 | loss dis xx = 19.9604 | loss dis zz = 1.5599\n",
      "Epoch 2 | time = 0s | loss gen = 39.7363 | loss enc = 42.6711 | loss dis = 29.8533 | loss dis xz = 12.0837 | loss dis xx = 16.1919 | loss dis zz = 1.5777\n",
      "Epoch 3 | time = 0s | loss gen = 38.9205 | loss enc = 41.2852 | loss dis = 27.1755 | loss dis xz = 11.8211 | loss dis xx = 13.8254 | loss dis zz = 1.5290\n",
      "Epoch 4 | time = 0s | loss gen = 40.4434 | loss enc = 42.4498 | loss dis = 24.1223 | loss dis xz = 11.4789 | loss dis xx = 11.0490 | loss dis zz = 1.5945\n",
      "Epoch 5 | time = 0s | loss gen = 42.7505 | loss enc = 45.3304 | loss dis = 23.1175 | loss dis xz = 12.1930 | loss dis xx = 9.2814 | loss dis zz = 1.6431\n",
      "Epoch 6 | time = 0s | loss gen = 46.3295 | loss enc = 48.8690 | loss dis = 21.1619 | loss dis xz = 10.3217 | loss dis xx = 9.3437 | loss dis zz = 1.4965\n",
      "Epoch 7 | time = 0s | loss gen = 48.3775 | loss enc = 51.3003 | loss dis = 19.7221 | loss dis xz = 10.5241 | loss dis xx = 7.6301 | loss dis zz = 1.5679\n",
      "Epoch 8 | time = 0s | loss gen = 48.8530 | loss enc = 51.7350 | loss dis = 19.4869 | loss dis xz = 10.6964 | loss dis xx = 7.1974 | loss dis zz = 1.5931\n",
      "Epoch 9 | time = 0s | loss gen = 46.6967 | loss enc = 50.1979 | loss dis = 18.9107 | loss dis xz = 9.9231 | loss dis xx = 7.3552 | loss dis zz = 1.6324\n",
      "Epoch 10 | time = 0s | loss gen = 49.7850 | loss enc = 53.1899 | loss dis = 17.1363 | loss dis xz = 9.1609 | loss dis xx = 6.3564 | loss dis zz = 1.6190\n",
      "Epoch 11 | time = 0s | loss gen = 53.0406 | loss enc = 56.2783 | loss dis = 19.0798 | loss dis xz = 10.3433 | loss dis xx = 7.1584 | loss dis zz = 1.5780\n",
      "Epoch 12 | time = 0s | loss gen = 52.2636 | loss enc = 55.0337 | loss dis = 16.5855 | loss dis xz = 9.3058 | loss dis xx = 5.7132 | loss dis zz = 1.5666\n",
      "Epoch 13 | time = 0s | loss gen = 49.0939 | loss enc = 53.3276 | loss dis = 18.1352 | loss dis xz = 10.2411 | loss dis xx = 6.3577 | loss dis zz = 1.5364\n",
      "Epoch 14 | time = 0s | loss gen = 49.7277 | loss enc = 54.6858 | loss dis = 18.4518 | loss dis xz = 9.8949 | loss dis xx = 6.9177 | loss dis zz = 1.6391\n",
      "Epoch 15 | time = 0s | loss gen = 49.7918 | loss enc = 53.6728 | loss dis = 16.6735 | loss dis xz = 8.4729 | loss dis xx = 6.7082 | loss dis zz = 1.4924\n",
      "Epoch 16 | time = 0s | loss gen = 51.9162 | loss enc = 57.4582 | loss dis = 14.8738 | loss dis xz = 8.1226 | loss dis xx = 5.2040 | loss dis zz = 1.5472\n",
      "Epoch 17 | time = 0s | loss gen = 53.9089 | loss enc = 59.4991 | loss dis = 14.7398 | loss dis xz = 7.7894 | loss dis xx = 5.3293 | loss dis zz = 1.6212\n",
      "Epoch 18 | time = 0s | loss gen = 55.0192 | loss enc = 61.7917 | loss dis = 14.7767 | loss dis xz = 8.4902 | loss dis xx = 4.6802 | loss dis zz = 1.6063\n",
      "Epoch 19 | time = 0s | loss gen = 56.3471 | loss enc = 62.6642 | loss dis = 13.2760 | loss dis xz = 7.4383 | loss dis xx = 4.2094 | loss dis zz = 1.6284\n",
      "Epoch 20 | time = 0s | loss gen = 56.0272 | loss enc = 63.9630 | loss dis = 13.6651 | loss dis xz = 7.0877 | loss dis xx = 5.0193 | loss dis zz = 1.5581\n",
      "Epoch 21 | time = 0s | loss gen = 59.1620 | loss enc = 68.2939 | loss dis = 12.4987 | loss dis xz = 6.4364 | loss dis xx = 4.4730 | loss dis zz = 1.5892\n",
      "Epoch 22 | time = 0s | loss gen = 57.2226 | loss enc = 64.9689 | loss dis = 10.4504 | loss dis xz = 5.5659 | loss dis xx = 3.3045 | loss dis zz = 1.5800\n",
      "Epoch 23 | time = 0s | loss gen = 58.5941 | loss enc = 69.5095 | loss dis = 12.7013 | loss dis xz = 5.7384 | loss dis xx = 5.3697 | loss dis zz = 1.5932\n",
      "Epoch 24 | time = 0s | loss gen = 57.1748 | loss enc = 67.3831 | loss dis = 11.6791 | loss dis xz = 6.2350 | loss dis xx = 3.8757 | loss dis zz = 1.5683\n",
      "Epoch 25 | time = 0s | loss gen = 57.8783 | loss enc = 72.0278 | loss dis = 12.1496 | loss dis xz = 5.6580 | loss dis xx = 4.9000 | loss dis zz = 1.5917\n",
      "Epoch 26 | time = 0s | loss gen = 63.6060 | loss enc = 75.6945 | loss dis = 12.2779 | loss dis xz = 5.9910 | loss dis xx = 4.7492 | loss dis zz = 1.5376\n",
      "Epoch 27 | time = 0s | loss gen = 62.0192 | loss enc = 73.4621 | loss dis = 10.2814 | loss dis xz = 4.5942 | loss dis xx = 4.1490 | loss dis zz = 1.5383\n",
      "Epoch 28 | time = 0s | loss gen = 59.3479 | loss enc = 70.0030 | loss dis = 9.6997 | loss dis xz = 4.9316 | loss dis xx = 3.2251 | loss dis zz = 1.5431\n",
      "Epoch 29 | time = 0s | loss gen = 62.6411 | loss enc = 75.8088 | loss dis = 10.7087 | loss dis xz = 6.0030 | loss dis xx = 3.0985 | loss dis zz = 1.6072\n",
      "Epoch 30 | time = 0s | loss gen = 61.6542 | loss enc = 74.9242 | loss dis = 9.6161 | loss dis xz = 4.0945 | loss dis xx = 3.9600 | loss dis zz = 1.5615\n",
      "Epoch 31 | time = 0s | loss gen = 63.3338 | loss enc = 78.2283 | loss dis = 9.2911 | loss dis xz = 3.8447 | loss dis xx = 3.8874 | loss dis zz = 1.5590\n",
      "Epoch 32 | time = 0s | loss gen = 62.1543 | loss enc = 76.3700 | loss dis = 8.2783 | loss dis xz = 4.2946 | loss dis xx = 2.4492 | loss dis zz = 1.5345\n",
      "Epoch 33 | time = 0s | loss gen = 66.3502 | loss enc = 79.6375 | loss dis = 10.8430 | loss dis xz = 5.6197 | loss dis xx = 3.7192 | loss dis zz = 1.5040\n",
      "Epoch 34 | time = 0s | loss gen = 60.8014 | loss enc = 73.8692 | loss dis = 8.0174 | loss dis xz = 3.7920 | loss dis xx = 2.6423 | loss dis zz = 1.5831\n",
      "Epoch 35 | time = 0s | loss gen = 66.7978 | loss enc = 82.2566 | loss dis = 8.2203 | loss dis xz = 4.0035 | loss dis xx = 2.6649 | loss dis zz = 1.5520\n",
      "Epoch 36 | time = 0s | loss gen = 63.9670 | loss enc = 80.0550 | loss dis = 8.5172 | loss dis xz = 4.3891 | loss dis xx = 2.5795 | loss dis zz = 1.5486\n",
      "Epoch 37 | time = 0s | loss gen = 61.2810 | loss enc = 78.5303 | loss dis = 8.2224 | loss dis xz = 3.7525 | loss dis xx = 2.9845 | loss dis zz = 1.4855\n",
      "Epoch 38 | time = 0s | loss gen = 62.9034 | loss enc = 79.0996 | loss dis = 7.6009 | loss dis xz = 3.0665 | loss dis xx = 3.0220 | loss dis zz = 1.5124\n",
      "Epoch 39 | time = 0s | loss gen = 61.9517 | loss enc = 77.6025 | loss dis = 8.4025 | loss dis xz = 3.4461 | loss dis xx = 3.4126 | loss dis zz = 1.5438\n",
      "Epoch 40 | time = 0s | loss gen = 62.0250 | loss enc = 79.6752 | loss dis = 8.6591 | loss dis xz = 4.3514 | loss dis xx = 2.7772 | loss dis zz = 1.5304\n",
      "Epoch 41 | time = 0s | loss gen = 65.2018 | loss enc = 80.8950 | loss dis = 7.0239 | loss dis xz = 2.8104 | loss dis xx = 2.6391 | loss dis zz = 1.5744\n",
      "Epoch 42 | time = 0s | loss gen = 66.9784 | loss enc = 83.1752 | loss dis = 8.5771 | loss dis xz = 3.2831 | loss dis xx = 3.7840 | loss dis zz = 1.5101\n",
      "Epoch 43 | time = 0s | loss gen = 65.7255 | loss enc = 83.3486 | loss dis = 7.5333 | loss dis xz = 3.7766 | loss dis xx = 2.1755 | loss dis zz = 1.5812\n",
      "Epoch 44 | time = 0s | loss gen = 62.1281 | loss enc = 81.0631 | loss dis = 8.0095 | loss dis xz = 2.5277 | loss dis xx = 3.9848 | loss dis zz = 1.4971\n",
      "Epoch 45 | time = 0s | loss gen = 67.6987 | loss enc = 84.7093 | loss dis = 7.2649 | loss dis xz = 2.7549 | loss dis xx = 2.9640 | loss dis zz = 1.5459\n",
      "Epoch 46 | time = 0s | loss gen = 65.4180 | loss enc = 87.8329 | loss dis = 7.4111 | loss dis xz = 3.3484 | loss dis xx = 2.5693 | loss dis zz = 1.4933\n",
      "Epoch 47 | time = 0s | loss gen = 68.1626 | loss enc = 88.1041 | loss dis = 7.4751 | loss dis xz = 2.7967 | loss dis xx = 3.1895 | loss dis zz = 1.4890\n",
      "Epoch 48 | time = 0s | loss gen = 64.4045 | loss enc = 85.6775 | loss dis = 8.1389 | loss dis xz = 3.4971 | loss dis xx = 3.1373 | loss dis zz = 1.5046\n",
      "Epoch 49 | time = 0s | loss gen = 64.4180 | loss enc = 85.7853 | loss dis = 8.5038 | loss dis xz = 3.7368 | loss dis xx = 3.2378 | loss dis zz = 1.5292\n",
      "Epoch 50 | time = 0s | loss gen = 61.7067 | loss enc = 83.1862 | loss dis = 7.6628 | loss dis xz = 3.4326 | loss dis xx = 2.7637 | loss dis zz = 1.4665\n",
      "Epoch 51 | time = 0s | loss gen = 65.9076 | loss enc = 87.5371 | loss dis = 8.3655 | loss dis xz = 3.7037 | loss dis xx = 3.1723 | loss dis zz = 1.4896\n",
      "Epoch 52 | time = 0s | loss gen = 63.7039 | loss enc = 83.6554 | loss dis = 7.5452 | loss dis xz = 3.0460 | loss dis xx = 3.0446 | loss dis zz = 1.4546\n",
      "Epoch 53 | time = 0s | loss gen = 62.3209 | loss enc = 86.1168 | loss dis = 8.1169 | loss dis xz = 3.0129 | loss dis xx = 3.6697 | loss dis zz = 1.4343\n",
      "Epoch 54 | time = 0s | loss gen = 63.6416 | loss enc = 86.5607 | loss dis = 8.6592 | loss dis xz = 4.0484 | loss dis xx = 3.1862 | loss dis zz = 1.4247\n",
      "Epoch 55 | time = 0s | loss gen = 60.7712 | loss enc = 85.2017 | loss dis = 8.0158 | loss dis xz = 4.0191 | loss dis xx = 2.5368 | loss dis zz = 1.4598\n",
      "Epoch 56 | time = 0s | loss gen = 62.3432 | loss enc = 89.5216 | loss dis = 7.8885 | loss dis xz = 3.6870 | loss dis xx = 2.7022 | loss dis zz = 1.4993\n",
      "Epoch 57 | time = 0s | loss gen = 57.1937 | loss enc = 81.1066 | loss dis = 7.8050 | loss dis xz = 4.1248 | loss dis xx = 2.2976 | loss dis zz = 1.3826\n",
      "Epoch 58 | time = 0s | loss gen = 61.2993 | loss enc = 85.0014 | loss dis = 10.2334 | loss dis xz = 5.1056 | loss dis xx = 3.7247 | loss dis zz = 1.4030\n",
      "Epoch 59 | time = 0s | loss gen = 61.6822 | loss enc = 87.4106 | loss dis = 9.9796 | loss dis xz = 4.8829 | loss dis xx = 3.6529 | loss dis zz = 1.4438\n",
      "Epoch 60 | time = 0s | loss gen = 64.9706 | loss enc = 91.9551 | loss dis = 8.9986 | loss dis xz = 3.7376 | loss dis xx = 3.8461 | loss dis zz = 1.4149\n",
      "Epoch 61 | time = 0s | loss gen = 64.4210 | loss enc = 94.1690 | loss dis = 7.6683 | loss dis xz = 3.6593 | loss dis xx = 2.6075 | loss dis zz = 1.4014\n",
      "Epoch 62 | time = 0s | loss gen = 63.4152 | loss enc = 92.6999 | loss dis = 9.3494 | loss dis xz = 4.7854 | loss dis xx = 3.1911 | loss dis zz = 1.3728\n",
      "Epoch 63 | time = 0s | loss gen = 59.1315 | loss enc = 89.7721 | loss dis = 9.1603 | loss dis xz = 4.3681 | loss dis xx = 3.3952 | loss dis zz = 1.3971\n",
      "Epoch 64 | time = 0s | loss gen = 60.0820 | loss enc = 94.4541 | loss dis = 9.6313 | loss dis xz = 4.8850 | loss dis xx = 3.3521 | loss dis zz = 1.3942\n",
      "Epoch 65 | time = 0s | loss gen = 56.9266 | loss enc = 86.8888 | loss dis = 8.5685 | loss dis xz = 4.1779 | loss dis xx = 3.0435 | loss dis zz = 1.3471\n",
      "Epoch 66 | time = 0s | loss gen = 59.3729 | loss enc = 93.1201 | loss dis = 11.1496 | loss dis xz = 5.5054 | loss dis xx = 4.2889 | loss dis zz = 1.3552\n",
      "Epoch 67 | time = 0s | loss gen = 56.3968 | loss enc = 85.3233 | loss dis = 10.7286 | loss dis xz = 5.8877 | loss dis xx = 3.4570 | loss dis zz = 1.3840\n",
      "Epoch 68 | time = 0s | loss gen = 61.4390 | loss enc = 93.7312 | loss dis = 10.0609 | loss dis xz = 4.8859 | loss dis xx = 3.8321 | loss dis zz = 1.3428\n",
      "Epoch 69 | time = 0s | loss gen = 63.2576 | loss enc = 94.1419 | loss dis = 11.1117 | loss dis xz = 6.6628 | loss dis xx = 3.1300 | loss dis zz = 1.3189\n",
      "Epoch 70 | time = 0s | loss gen = 59.8763 | loss enc = 87.8837 | loss dis = 13.8456 | loss dis xz = 8.0672 | loss dis xx = 4.4528 | loss dis zz = 1.3256\n",
      "Epoch 71 | time = 0s | loss gen = 63.7727 | loss enc = 95.1136 | loss dis = 12.0008 | loss dis xz = 6.9807 | loss dis xx = 3.6883 | loss dis zz = 1.3317\n",
      "Epoch 72 | time = 0s | loss gen = 66.8900 | loss enc = 97.7985 | loss dis = 11.0099 | loss dis xz = 5.4218 | loss dis xx = 4.2478 | loss dis zz = 1.3404\n",
      "Epoch 73 | time = 0s | loss gen = 62.3862 | loss enc = 94.5304 | loss dis = 12.1281 | loss dis xz = 7.1644 | loss dis xx = 3.6322 | loss dis zz = 1.3316\n",
      "Epoch 74 | time = 0s | loss gen = 64.6472 | loss enc = 97.9401 | loss dis = 13.2661 | loss dis xz = 8.5435 | loss dis xx = 3.3560 | loss dis zz = 1.3666\n",
      "Epoch 75 | time = 0s | loss gen = 58.5324 | loss enc = 88.9574 | loss dis = 12.3719 | loss dis xz = 7.0551 | loss dis xx = 3.9827 | loss dis zz = 1.3341\n",
      "Epoch 76 | time = 0s | loss gen = 64.7779 | loss enc = 94.4876 | loss dis = 13.4726 | loss dis xz = 7.8278 | loss dis xx = 4.3070 | loss dis zz = 1.3378\n",
      "Epoch 77 | time = 0s | loss gen = 63.4273 | loss enc = 94.4175 | loss dis = 11.1763 | loss dis xz = 6.9051 | loss dis xx = 2.9737 | loss dis zz = 1.2975\n",
      "Epoch 78 | time = 0s | loss gen = 64.9957 | loss enc = 99.0364 | loss dis = 12.9356 | loss dis xz = 7.8716 | loss dis xx = 3.7943 | loss dis zz = 1.2696\n",
      "Epoch 79 | time = 0s | loss gen = 63.2643 | loss enc = 100.0777 | loss dis = 10.3088 | loss dis xz = 5.9352 | loss dis xx = 3.0779 | loss dis zz = 1.2957\n",
      "Epoch 80 | time = 0s | loss gen = 63.7789 | loss enc = 102.0796 | loss dis = 14.1025 | loss dis xz = 7.1126 | loss dis xx = 5.6917 | loss dis zz = 1.2981\n",
      "Epoch 81 | time = 0s | loss gen = 68.8273 | loss enc = 100.8510 | loss dis = 11.9439 | loss dis xz = 5.7160 | loss dis xx = 4.9769 | loss dis zz = 1.2510\n",
      "Epoch 82 | time = 0s | loss gen = 62.5479 | loss enc = 97.9731 | loss dis = 11.8201 | loss dis xz = 6.7214 | loss dis xx = 3.8097 | loss dis zz = 1.2890\n",
      "Epoch 83 | time = 0s | loss gen = 65.0929 | loss enc = 103.7984 | loss dis = 12.6738 | loss dis xz = 6.3652 | loss dis xx = 4.9646 | loss dis zz = 1.3439\n",
      "Epoch 84 | time = 0s | loss gen = 69.6079 | loss enc = 105.3952 | loss dis = 12.2940 | loss dis xz = 6.8708 | loss dis xx = 4.1662 | loss dis zz = 1.2570\n",
      "Epoch 85 | time = 0s | loss gen = 62.7332 | loss enc = 99.7711 | loss dis = 12.3436 | loss dis xz = 7.0369 | loss dis xx = 4.0392 | loss dis zz = 1.2675\n",
      "Epoch 86 | time = 0s | loss gen = 63.6278 | loss enc = 102.0901 | loss dis = 10.1161 | loss dis xz = 4.6522 | loss dis xx = 4.1865 | loss dis zz = 1.2774\n",
      "Epoch 87 | time = 0s | loss gen = 67.2741 | loss enc = 104.0426 | loss dis = 13.1440 | loss dis xz = 8.3266 | loss dis xx = 3.4597 | loss dis zz = 1.3576\n",
      "Epoch 88 | time = 0s | loss gen = 65.0804 | loss enc = 108.6370 | loss dis = 12.2238 | loss dis xz = 7.1507 | loss dis xx = 3.7639 | loss dis zz = 1.3091\n",
      "Epoch 89 | time = 0s | loss gen = 65.0573 | loss enc = 111.8247 | loss dis = 12.4322 | loss dis xz = 6.4948 | loss dis xx = 4.6855 | loss dis zz = 1.2520\n",
      "Epoch 90 | time = 0s | loss gen = 62.3554 | loss enc = 102.9584 | loss dis = 12.8569 | loss dis xz = 7.8980 | loss dis xx = 3.6562 | loss dis zz = 1.3027\n",
      "Epoch 91 | time = 0s | loss gen = 66.8161 | loss enc = 110.6314 | loss dis = 15.4202 | loss dis xz = 8.0918 | loss dis xx = 5.9964 | loss dis zz = 1.3321\n",
      "Epoch 92 | time = 0s | loss gen = 71.7742 | loss enc = 120.6939 | loss dis = 11.5283 | loss dis xz = 6.5216 | loss dis xx = 3.7362 | loss dis zz = 1.2705\n",
      "INFO:tensorflow:Saving checkpoint to path train_logs/arrhythmia/alad_snFalse_dzzTrue/dzzenabledTrue//label1/rd2/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoint to path train_logs/arrhythmia/alad_snFalse_dzzTrue/dzzenabledTrue//label1/rd2/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 | time = 0s | loss gen = 66.6306 | loss enc = 116.0867 | loss dis = 8.7541 | loss dis xz = 4.3741 | loss dis xx = 3.0785 | loss dis zz = 1.3015\n",
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Tensor' object has no attribute 'to_proto'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Tensor' object has no attribute 'to_proto'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 | time = 0s | loss gen = 63.3966 | loss enc = 112.8310 | loss dis = 10.2980 | loss dis xz = 4.3879 | loss dis xx = 4.5991 | loss dis zz = 1.3111\n",
      "Epoch 95 | time = 0s | loss gen = 70.5145 | loss enc = 116.1893 | loss dis = 11.3108 | loss dis xz = 6.6592 | loss dis xx = 3.3956 | loss dis zz = 1.2560\n",
      "Epoch 96 | time = 0s | loss gen = 68.9116 | loss enc = 115.8013 | loss dis = 9.6800 | loss dis xz = 4.0872 | loss dis xx = 4.3144 | loss dis zz = 1.2784\n",
      "Epoch 97 | time = 0s | loss gen = 67.6053 | loss enc = 118.9799 | loss dis = 10.6920 | loss dis xz = 5.1326 | loss dis xx = 4.2835 | loss dis zz = 1.2759\n",
      "Epoch 98 | time = 0s | loss gen = 71.7410 | loss enc = 122.1075 | loss dis = 10.8813 | loss dis xz = 5.4859 | loss dis xx = 4.1443 | loss dis zz = 1.2510\n",
      "Epoch 99 | time = 0s | loss gen = 68.7655 | loss enc = 121.8422 | loss dis = 9.5646 | loss dis xz = 4.5966 | loss dis xx = 3.6992 | loss dis zz = 1.2689\n",
      "Epoch 100 | time = 0s | loss gen = 75.1119 | loss enc = 130.0923 | loss dis = 9.7666 | loss dis xz = 4.8310 | loss dis xx = 3.7091 | loss dis zz = 1.2264\n",
      "Epoch 101 | time = 0s | loss gen = 76.7813 | loss enc = 133.6384 | loss dis = 8.9557 | loss dis xz = 4.9730 | loss dis xx = 2.6949 | loss dis zz = 1.2878\n",
      "Epoch 102 | time = 0s | loss gen = 68.9417 | loss enc = 120.2777 | loss dis = 8.4083 | loss dis xz = 3.1424 | loss dis xx = 3.9671 | loss dis zz = 1.2988\n",
      "Epoch 103 | time = 0s | loss gen = 71.5775 | loss enc = 124.6504 | loss dis = 8.3448 | loss dis xz = 3.6982 | loss dis xx = 3.4098 | loss dis zz = 1.2367\n",
      "Epoch 104 | time = 0s | loss gen = 68.6147 | loss enc = 119.9352 | loss dis = 8.0659 | loss dis xz = 3.9897 | loss dis xx = 2.8517 | loss dis zz = 1.2245\n",
      "Epoch 105 | time = 0s | loss gen = 70.8288 | loss enc = 127.8631 | loss dis = 10.7697 | loss dis xz = 5.3930 | loss dis xx = 4.1019 | loss dis zz = 1.2748\n",
      "Epoch 106 | time = 0s | loss gen = 67.2546 | loss enc = 125.3935 | loss dis = 8.2046 | loss dis xz = 3.1060 | loss dis xx = 3.8357 | loss dis zz = 1.2630\n",
      "Epoch 107 | time = 0s | loss gen = 66.9999 | loss enc = 122.9946 | loss dis = 9.5453 | loss dis xz = 4.0594 | loss dis xx = 4.2554 | loss dis zz = 1.2305\n",
      "Epoch 108 | time = 0s | loss gen = 65.0204 | loss enc = 126.4778 | loss dis = 8.5830 | loss dis xz = 4.4409 | loss dis xx = 2.8758 | loss dis zz = 1.2663\n",
      "Epoch 109 | time = 0s | loss gen = 57.5482 | loss enc = 114.6568 | loss dis = 8.7482 | loss dis xz = 3.5069 | loss dis xx = 3.9334 | loss dis zz = 1.3079\n",
      "Epoch 110 | time = 0s | loss gen = 64.7274 | loss enc = 119.9386 | loss dis = 10.3289 | loss dis xz = 3.4809 | loss dis xx = 5.6117 | loss dis zz = 1.2363\n",
      "Epoch 111 | time = 0s | loss gen = 64.5388 | loss enc = 123.5232 | loss dis = 9.0203 | loss dis xz = 3.4135 | loss dis xx = 4.3570 | loss dis zz = 1.2498\n",
      "Epoch 112 | time = 0s | loss gen = 65.6043 | loss enc = 124.1759 | loss dis = 11.7641 | loss dis xz = 5.2107 | loss dis xx = 5.3041 | loss dis zz = 1.2494\n",
      "Epoch 113 | time = 0s | loss gen = 62.7776 | loss enc = 118.1759 | loss dis = 8.3045 | loss dis xz = 2.1713 | loss dis xx = 4.8661 | loss dis zz = 1.2671\n",
      "Epoch 114 | time = 0s | loss gen = 62.8029 | loss enc = 124.6923 | loss dis = 8.4734 | loss dis xz = 2.1977 | loss dis xx = 5.0218 | loss dis zz = 1.2540\n",
      "Epoch 115 | time = 0s | loss gen = 66.8797 | loss enc = 123.6292 | loss dis = 9.1383 | loss dis xz = 3.2053 | loss dis xx = 4.6768 | loss dis zz = 1.2562\n",
      "Epoch 116 | time = 0s | loss gen = 62.8523 | loss enc = 115.2690 | loss dis = 10.1344 | loss dis xz = 3.5361 | loss dis xx = 5.4067 | loss dis zz = 1.1916\n",
      "Epoch 117 | time = 0s | loss gen = 60.0609 | loss enc = 119.2836 | loss dis = 8.8877 | loss dis xz = 2.7638 | loss dis xx = 4.8891 | loss dis zz = 1.2348\n",
      "Epoch 118 | time = 0s | loss gen = 59.0585 | loss enc = 116.3424 | loss dis = 8.2257 | loss dis xz = 2.0376 | loss dis xx = 4.9095 | loss dis zz = 1.2786\n",
      "Epoch 119 | time = 0s | loss gen = 58.3732 | loss enc = 115.9050 | loss dis = 9.2775 | loss dis xz = 2.7989 | loss dis xx = 5.2616 | loss dis zz = 1.2170\n",
      "Epoch 120 | time = 0s | loss gen = 61.1801 | loss enc = 118.4057 | loss dis = 8.1574 | loss dis xz = 2.2142 | loss dis xx = 4.6792 | loss dis zz = 1.2640\n",
      "Epoch 121 | time = 0s | loss gen = 57.7160 | loss enc = 115.4924 | loss dis = 8.7511 | loss dis xz = 2.7474 | loss dis xx = 4.7899 | loss dis zz = 1.2137\n",
      "Epoch 122 | time = 0s | loss gen = 56.5024 | loss enc = 114.8475 | loss dis = 11.2184 | loss dis xz = 3.2399 | loss dis xx = 6.6980 | loss dis zz = 1.2806\n",
      "Epoch 123 | time = 0s | loss gen = 57.5183 | loss enc = 119.1523 | loss dis = 8.9965 | loss dis xz = 3.3438 | loss dis xx = 4.4936 | loss dis zz = 1.1590\n",
      "Epoch 124 | time = 0s | loss gen = 59.5286 | loss enc = 114.6745 | loss dis = 9.4157 | loss dis xz = 2.0727 | loss dis xx = 6.1222 | loss dis zz = 1.2208\n",
      "Epoch 125 | time = 0s | loss gen = 56.8468 | loss enc = 114.9951 | loss dis = 10.1781 | loss dis xz = 2.8510 | loss dis xx = 6.1083 | loss dis zz = 1.2187\n",
      "Epoch 126 | time = 0s | loss gen = 54.8024 | loss enc = 110.9980 | loss dis = 10.1959 | loss dis xz = 3.4741 | loss dis xx = 5.4908 | loss dis zz = 1.2310\n",
      "Epoch 127 | time = 0s | loss gen = 51.9952 | loss enc = 110.9122 | loss dis = 8.8146 | loss dis xz = 2.1964 | loss dis xx = 5.4012 | loss dis zz = 1.2171\n",
      "Epoch 128 | time = 0s | loss gen = 55.5582 | loss enc = 109.6999 | loss dis = 7.9800 | loss dis xz = 2.5557 | loss dis xx = 4.2039 | loss dis zz = 1.2204\n",
      "Epoch 129 | time = 0s | loss gen = 50.7756 | loss enc = 101.7438 | loss dis = 8.9191 | loss dis xz = 2.8349 | loss dis xx = 4.9046 | loss dis zz = 1.1796\n",
      "Epoch 130 | time = 0s | loss gen = 54.5162 | loss enc = 103.9705 | loss dis = 9.6341 | loss dis xz = 2.9767 | loss dis xx = 5.4165 | loss dis zz = 1.2408\n",
      "Epoch 131 | time = 0s | loss gen = 51.5987 | loss enc = 103.8317 | loss dis = 9.2677 | loss dis xz = 3.0377 | loss dis xx = 5.0130 | loss dis zz = 1.2170\n",
      "Epoch 132 | time = 0s | loss gen = 52.7680 | loss enc = 100.8910 | loss dis = 9.8459 | loss dis xz = 3.9672 | loss dis xx = 4.6438 | loss dis zz = 1.2350\n",
      "Epoch 133 | time = 0s | loss gen = 52.2288 | loss enc = 103.3862 | loss dis = 9.5906 | loss dis xz = 2.8171 | loss dis xx = 5.5448 | loss dis zz = 1.2287\n",
      "Epoch 134 | time = 0s | loss gen = 53.2359 | loss enc = 104.4788 | loss dis = 9.3363 | loss dis xz = 3.4431 | loss dis xx = 4.6473 | loss dis zz = 1.2460\n",
      "Epoch 135 | time = 0s | loss gen = 48.3278 | loss enc = 93.7441 | loss dis = 9.8294 | loss dis xz = 3.5925 | loss dis xx = 4.9908 | loss dis zz = 1.2461\n",
      "Epoch 136 | time = 0s | loss gen = 49.6940 | loss enc = 97.3322 | loss dis = 9.5208 | loss dis xz = 2.8132 | loss dis xx = 5.5096 | loss dis zz = 1.1980\n",
      "Epoch 137 | time = 0s | loss gen = 45.9135 | loss enc = 90.5804 | loss dis = 10.4556 | loss dis xz = 4.1452 | loss dis xx = 5.1119 | loss dis zz = 1.1985\n",
      "Epoch 138 | time = 0s | loss gen = 51.3652 | loss enc = 99.0052 | loss dis = 8.0533 | loss dis xz = 2.4804 | loss dis xx = 4.3855 | loss dis zz = 1.1874\n",
      "Epoch 139 | time = 0s | loss gen = 47.8817 | loss enc = 94.1191 | loss dis = 10.1318 | loss dis xz = 2.7892 | loss dis xx = 6.0559 | loss dis zz = 1.2867\n",
      "Epoch 140 | time = 0s | loss gen = 50.0566 | loss enc = 94.5001 | loss dis = 10.0958 | loss dis xz = 4.0833 | loss dis xx = 4.7146 | loss dis zz = 1.2979\n",
      "Epoch 141 | time = 0s | loss gen = 45.9266 | loss enc = 88.3129 | loss dis = 8.2846 | loss dis xz = 3.0556 | loss dis xx = 3.9732 | loss dis zz = 1.2558\n",
      "Epoch 142 | time = 0s | loss gen = 43.5143 | loss enc = 92.0118 | loss dis = 9.7136 | loss dis xz = 4.1303 | loss dis xx = 4.3120 | loss dis zz = 1.2713\n",
      "Epoch 143 | time = 0s | loss gen = 44.7408 | loss enc = 92.5091 | loss dis = 8.8194 | loss dis xz = 3.3104 | loss dis xx = 4.2290 | loss dis zz = 1.2799\n",
      "Epoch 144 | time = 0s | loss gen = 44.6388 | loss enc = 93.1480 | loss dis = 10.1033 | loss dis xz = 4.5383 | loss dis xx = 4.3800 | loss dis zz = 1.1851\n",
      "Epoch 145 | time = 0s | loss gen = 45.4512 | loss enc = 92.4368 | loss dis = 9.2101 | loss dis xz = 2.3016 | loss dis xx = 5.6566 | loss dis zz = 1.2519\n",
      "Epoch 146 | time = 0s | loss gen = 47.8473 | loss enc = 92.1444 | loss dis = 8.9607 | loss dis xz = 2.3204 | loss dis xx = 5.3946 | loss dis zz = 1.2456\n",
      "Epoch 147 | time = 0s | loss gen = 45.8602 | loss enc = 94.9915 | loss dis = 9.3638 | loss dis xz = 3.5940 | loss dis xx = 4.5755 | loss dis zz = 1.1943\n",
      "Epoch 148 | time = 0s | loss gen = 48.0696 | loss enc = 92.9242 | loss dis = 8.0483 | loss dis xz = 2.0251 | loss dis xx = 4.7315 | loss dis zz = 1.2916\n",
      "Epoch 149 | time = 0s | loss gen = 47.7473 | loss enc = 96.8685 | loss dis = 9.9971 | loss dis xz = 2.8527 | loss dis xx = 5.8325 | loss dis zz = 1.3119\n",
      "Epoch 150 | time = 0s | loss gen = 49.1784 | loss enc = 93.5450 | loss dis = 8.7255 | loss dis xz = 3.5232 | loss dis xx = 3.9163 | loss dis zz = 1.2860\n",
      "Epoch 151 | time = 0s | loss gen = 46.1447 | loss enc = 93.4666 | loss dis = 9.6233 | loss dis xz = 3.6337 | loss dis xx = 4.7394 | loss dis zz = 1.2502\n",
      "Epoch 152 | time = 0s | loss gen = 45.7848 | loss enc = 87.1055 | loss dis = 8.7420 | loss dis xz = 3.1326 | loss dis xx = 4.3174 | loss dis zz = 1.2920\n",
      "Epoch 153 | time = 0s | loss gen = 47.1959 | loss enc = 95.1249 | loss dis = 9.0270 | loss dis xz = 4.1022 | loss dis xx = 3.6588 | loss dis zz = 1.2660\n",
      "Epoch 154 | time = 0s | loss gen = 44.9678 | loss enc = 94.7164 | loss dis = 7.8474 | loss dis xz = 3.0912 | loss dis xx = 3.4657 | loss dis zz = 1.2905\n",
      "Epoch 155 | time = 0s | loss gen = 49.8875 | loss enc = 98.9456 | loss dis = 7.2076 | loss dis xz = 2.2407 | loss dis xx = 3.6491 | loss dis zz = 1.3178\n",
      "Epoch 156 | time = 0s | loss gen = 44.2321 | loss enc = 92.2598 | loss dis = 11.5608 | loss dis xz = 5.5117 | loss dis xx = 4.7685 | loss dis zz = 1.2806\n",
      "Epoch 157 | time = 0s | loss gen = 46.4983 | loss enc = 93.5922 | loss dis = 7.5384 | loss dis xz = 2.5893 | loss dis xx = 3.5889 | loss dis zz = 1.3603\n",
      "Epoch 158 | time = 0s | loss gen = 48.3068 | loss enc = 95.6420 | loss dis = 8.4977 | loss dis xz = 3.3250 | loss dis xx = 3.8531 | loss dis zz = 1.3195\n",
      "Epoch 159 | time = 0s | loss gen = 47.8949 | loss enc = 100.3884 | loss dis = 7.5998 | loss dis xz = 2.5507 | loss dis xx = 3.7001 | loss dis zz = 1.3490\n",
      "Epoch 160 | time = 0s | loss gen = 46.0705 | loss enc = 96.7415 | loss dis = 8.5214 | loss dis xz = 3.3888 | loss dis xx = 3.8353 | loss dis zz = 1.2974\n",
      "Epoch 161 | time = 0s | loss gen = 44.7502 | loss enc = 93.1842 | loss dis = 7.1685 | loss dis xz = 2.7410 | loss dis xx = 3.1864 | loss dis zz = 1.2411\n",
      "Epoch 162 | time = 0s | loss gen = 45.2759 | loss enc = 97.5148 | loss dis = 8.8406 | loss dis xz = 3.4396 | loss dis xx = 4.0988 | loss dis zz = 1.3023\n",
      "Epoch 163 | time = 0s | loss gen = 46.3593 | loss enc = 96.1031 | loss dis = 8.0085 | loss dis xz = 3.2709 | loss dis xx = 3.3824 | loss dis zz = 1.3551\n",
      "Epoch 164 | time = 0s | loss gen = 44.0325 | loss enc = 92.7487 | loss dis = 6.3674 | loss dis xz = 1.7949 | loss dis xx = 3.3014 | loss dis zz = 1.2712\n",
      "Epoch 165 | time = 0s | loss gen = 40.8315 | loss enc = 88.9221 | loss dis = 8.2248 | loss dis xz = 3.0657 | loss dis xx = 3.8749 | loss dis zz = 1.2842\n",
      "Epoch 166 | time = 0s | loss gen = 43.0679 | loss enc = 92.4591 | loss dis = 7.4594 | loss dis xz = 2.4173 | loss dis xx = 3.7057 | loss dis zz = 1.3363\n",
      "Epoch 167 | time = 0s | loss gen = 43.5892 | loss enc = 90.0965 | loss dis = 8.4462 | loss dis xz = 2.9377 | loss dis xx = 4.1384 | loss dis zz = 1.3701\n",
      "Epoch 168 | time = 0s | loss gen = 44.2740 | loss enc = 94.2636 | loss dis = 8.8793 | loss dis xz = 2.9202 | loss dis xx = 4.6379 | loss dis zz = 1.3212\n",
      "Epoch 169 | time = 0s | loss gen = 42.0873 | loss enc = 97.2962 | loss dis = 8.7205 | loss dis xz = 3.1088 | loss dis xx = 4.2792 | loss dis zz = 1.3325\n",
      "Epoch 170 | time = 0s | loss gen = 40.7828 | loss enc = 95.4003 | loss dis = 8.0865 | loss dis xz = 2.9503 | loss dis xx = 3.8375 | loss dis zz = 1.2988\n",
      "Epoch 171 | time = 0s | loss gen = 40.8152 | loss enc = 90.2938 | loss dis = 6.2989 | loss dis xz = 1.3107 | loss dis xx = 3.7014 | loss dis zz = 1.2868\n",
      "Epoch 172 | time = 0s | loss gen = 42.1910 | loss enc = 95.4274 | loss dis = 7.1616 | loss dis xz = 1.7889 | loss dis xx = 4.0323 | loss dis zz = 1.3404\n",
      "Epoch 173 | time = 0s | loss gen = 40.1675 | loss enc = 88.7333 | loss dis = 8.6585 | loss dis xz = 3.1325 | loss dis xx = 4.2382 | loss dis zz = 1.2878\n",
      "Epoch 174 | time = 0s | loss gen = 39.2510 | loss enc = 86.5591 | loss dis = 8.1187 | loss dis xz = 2.1224 | loss dis xx = 4.6507 | loss dis zz = 1.3456\n",
      "Epoch 175 | time = 0s | loss gen = 41.6845 | loss enc = 93.5709 | loss dis = 8.0351 | loss dis xz = 2.3954 | loss dis xx = 4.3827 | loss dis zz = 1.2570\n",
      "Epoch 176 | time = 0s | loss gen = 37.0097 | loss enc = 89.5582 | loss dis = 7.8958 | loss dis xz = 2.0957 | loss dis xx = 4.4799 | loss dis zz = 1.3203\n",
      "Epoch 177 | time = 0s | loss gen = 38.0649 | loss enc = 89.9437 | loss dis = 7.4007 | loss dis xz = 1.5809 | loss dis xx = 4.4855 | loss dis zz = 1.3343\n",
      "Epoch 178 | time = 0s | loss gen = 41.9850 | loss enc = 93.7765 | loss dis = 8.2343 | loss dis xz = 2.1902 | loss dis xx = 4.6879 | loss dis zz = 1.3562\n",
      "Epoch 179 | time = 0s | loss gen = 38.2803 | loss enc = 89.3591 | loss dis = 7.9836 | loss dis xz = 2.0873 | loss dis xx = 4.6081 | loss dis zz = 1.2883\n",
      "Epoch 180 | time = 0s | loss gen = 38.6608 | loss enc = 90.3763 | loss dis = 7.7195 | loss dis xz = 2.0353 | loss dis xx = 4.3655 | loss dis zz = 1.3187\n",
      "Epoch 181 | time = 0s | loss gen = 37.6164 | loss enc = 87.3326 | loss dis = 7.4032 | loss dis xz = 2.0671 | loss dis xx = 3.9908 | loss dis zz = 1.3453\n",
      "Epoch 182 | time = 0s | loss gen = 35.2551 | loss enc = 83.6448 | loss dis = 7.7470 | loss dis xz = 1.7729 | loss dis xx = 4.5997 | loss dis zz = 1.3745\n",
      "Epoch 183 | time = 0s | loss gen = 39.5745 | loss enc = 89.9060 | loss dis = 8.2340 | loss dis xz = 1.3670 | loss dis xx = 5.5651 | loss dis zz = 1.3019\n",
      "Epoch 184 | time = 0s | loss gen = 38.9455 | loss enc = 84.8726 | loss dis = 8.1817 | loss dis xz = 3.0908 | loss dis xx = 3.7982 | loss dis zz = 1.2927\n",
      "Epoch 185 | time = 0s | loss gen = 38.0757 | loss enc = 89.5280 | loss dis = 8.2947 | loss dis xz = 2.0511 | loss dis xx = 4.8827 | loss dis zz = 1.3609\n",
      "Epoch 186 | time = 0s | loss gen = 37.7179 | loss enc = 91.6710 | loss dis = 9.0845 | loss dis xz = 2.9349 | loss dis xx = 4.8289 | loss dis zz = 1.3207\n",
      "Epoch 187 | time = 0s | loss gen = 37.0137 | loss enc = 86.0093 | loss dis = 6.2963 | loss dis xz = 1.1739 | loss dis xx = 3.7921 | loss dis zz = 1.3304\n",
      "Epoch 188 | time = 0s | loss gen = 37.5624 | loss enc = 87.1866 | loss dis = 7.6434 | loss dis xz = 2.1515 | loss dis xx = 4.1169 | loss dis zz = 1.3750\n",
      "Epoch 189 | time = 0s | loss gen = 37.0963 | loss enc = 90.8268 | loss dis = 7.3063 | loss dis xz = 1.6008 | loss dis xx = 4.3519 | loss dis zz = 1.3536\n",
      "Epoch 190 | time = 0s | loss gen = 36.6046 | loss enc = 84.6952 | loss dis = 7.9889 | loss dis xz = 1.8944 | loss dis xx = 4.7830 | loss dis zz = 1.3116\n",
      "Epoch 191 | time = 0s | loss gen = 36.6921 | loss enc = 84.2602 | loss dis = 7.0848 | loss dis xz = 0.9086 | loss dis xx = 4.8870 | loss dis zz = 1.2893\n",
      "Epoch 192 | time = 0s | loss gen = 37.1770 | loss enc = 88.0857 | loss dis = 8.0620 | loss dis xz = 2.4679 | loss dis xx = 4.2107 | loss dis zz = 1.3834\n",
      "Epoch 193 | time = 0s | loss gen = 36.3849 | loss enc = 85.4688 | loss dis = 8.2218 | loss dis xz = 2.8616 | loss dis xx = 3.9730 | loss dis zz = 1.3872\n",
      "Epoch 194 | time = 0s | loss gen = 35.9704 | loss enc = 84.3490 | loss dis = 7.1085 | loss dis xz = 1.5984 | loss dis xx = 4.1885 | loss dis zz = 1.3217\n",
      "Epoch 195 | time = 0s | loss gen = 35.7653 | loss enc = 84.5417 | loss dis = 7.6136 | loss dis xz = 1.1116 | loss dis xx = 5.1638 | loss dis zz = 1.3381\n",
      "Epoch 196 | time = 0s | loss gen = 35.5437 | loss enc = 90.5206 | loss dis = 7.4513 | loss dis xz = 2.2411 | loss dis xx = 3.8376 | loss dis zz = 1.3726\n",
      "Epoch 197 | time = 0s | loss gen = 37.0229 | loss enc = 86.9223 | loss dis = 8.3047 | loss dis xz = 1.9465 | loss dis xx = 5.0342 | loss dis zz = 1.3240\n",
      "Epoch 198 | time = 0s | loss gen = 36.8782 | loss enc = 84.9958 | loss dis = 7.7580 | loss dis xz = 1.5435 | loss dis xx = 4.8884 | loss dis zz = 1.3261\n",
      "Epoch 199 | time = 0s | loss gen = 35.9331 | loss enc = 83.9382 | loss dis = 8.3393 | loss dis xz = 1.6777 | loss dis xx = 5.2954 | loss dis zz = 1.3662\n",
      "Epoch 200 | time = 0s | loss gen = 36.2181 | loss enc = 85.5710 | loss dis = 7.8063 | loss dis xz = 2.1645 | loss dis xx = 4.2233 | loss dis zz = 1.4185\n",
      "Epoch 201 | time = 0s | loss gen = 33.0775 | loss enc = 78.7154 | loss dis = 7.2365 | loss dis xz = 1.8544 | loss dis xx = 4.0518 | loss dis zz = 1.3303\n",
      "Epoch 202 | time = 0s | loss gen = 34.9353 | loss enc = 80.0149 | loss dis = 7.6426 | loss dis xz = 1.8923 | loss dis xx = 4.4157 | loss dis zz = 1.3346\n",
      "Epoch 203 | time = 0s | loss gen = 32.6053 | loss enc = 78.0584 | loss dis = 7.7293 | loss dis xz = 1.5283 | loss dis xx = 4.8712 | loss dis zz = 1.3297\n",
      "INFO:tensorflow:Saving checkpoint to path train_logs/arrhythmia/alad_snFalse_dzzTrue/dzzenabledTrue//label1/rd2/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoint to path train_logs/arrhythmia/alad_snFalse_dzzTrue/dzzenabledTrue//label1/rd2/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204 | time = 0s | loss gen = 32.6978 | loss enc = 78.1181 | loss dis = 7.9960 | loss dis xz = 2.2360 | loss dis xx = 4.4118 | loss dis zz = 1.3482\n",
      "0 % epoch\rWARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Tensor' object has no attribute 'to_proto'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Tensor' object has no attribute 'to_proto'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205 | time = 0s | loss gen = 33.3521 | loss enc = 78.4146 | loss dis = 8.8953 | loss dis xz = 2.0697 | loss dis xx = 5.4677 | loss dis zz = 1.3579\n",
      "Epoch 206 | time = 0s | loss gen = 33.7258 | loss enc = 80.3891 | loss dis = 8.1030 | loss dis xz = 1.9924 | loss dis xx = 4.7637 | loss dis zz = 1.3469\n",
      "Epoch 207 | time = 0s | loss gen = 33.5396 | loss enc = 83.7589 | loss dis = 9.1857 | loss dis xz = 2.3228 | loss dis xx = 5.5077 | loss dis zz = 1.3553\n",
      "Epoch 208 | time = 0s | loss gen = 30.7630 | loss enc = 76.8676 | loss dis = 8.3929 | loss dis xz = 2.2863 | loss dis xx = 4.6928 | loss dis zz = 1.4138\n",
      "Epoch 209 | time = 0s | loss gen = 32.9469 | loss enc = 84.9399 | loss dis = 9.6048 | loss dis xz = 2.8003 | loss dis xx = 5.3994 | loss dis zz = 1.4051\n",
      "Epoch 210 | time = 0s | loss gen = 30.1891 | loss enc = 79.2976 | loss dis = 9.0178 | loss dis xz = 1.9679 | loss dis xx = 5.7034 | loss dis zz = 1.3465\n",
      "Epoch 211 | time = 0s | loss gen = 31.1265 | loss enc = 77.8713 | loss dis = 8.8202 | loss dis xz = 2.0536 | loss dis xx = 5.4504 | loss dis zz = 1.3162\n",
      "Epoch 212 | time = 0s | loss gen = 32.3649 | loss enc = 80.2106 | loss dis = 8.8384 | loss dis xz = 1.6873 | loss dis xx = 5.7747 | loss dis zz = 1.3764\n",
      "Epoch 213 | time = 0s | loss gen = 31.7517 | loss enc = 83.2761 | loss dis = 10.0881 | loss dis xz = 2.1325 | loss dis xx = 6.6248 | loss dis zz = 1.3308\n",
      "Epoch 214 | time = 0s | loss gen = 30.4584 | loss enc = 75.0719 | loss dis = 8.8216 | loss dis xz = 1.8820 | loss dis xx = 5.5555 | loss dis zz = 1.3842\n",
      "Epoch 215 | time = 0s | loss gen = 32.0799 | loss enc = 80.4660 | loss dis = 10.3521 | loss dis xz = 2.1044 | loss dis xx = 6.8753 | loss dis zz = 1.3724\n",
      "Epoch 216 | time = 0s | loss gen = 32.9301 | loss enc = 80.1545 | loss dis = 7.6921 | loss dis xz = 1.3308 | loss dis xx = 4.9804 | loss dis zz = 1.3809\n",
      "Epoch 217 | time = 0s | loss gen = 31.1000 | loss enc = 78.5118 | loss dis = 10.2798 | loss dis xz = 2.5264 | loss dis xx = 6.3766 | loss dis zz = 1.3768\n",
      "Epoch 218 | time = 0s | loss gen = 32.3261 | loss enc = 82.7286 | loss dis = 8.8829 | loss dis xz = 2.2715 | loss dis xx = 5.2526 | loss dis zz = 1.3588\n",
      "Epoch 219 | time = 0s | loss gen = 31.3310 | loss enc = 82.8817 | loss dis = 8.4892 | loss dis xz = 1.6212 | loss dis xx = 5.4923 | loss dis zz = 1.3757\n",
      "Epoch 220 | time = 0s | loss gen = 31.0697 | loss enc = 77.9568 | loss dis = 10.7242 | loss dis xz = 2.6937 | loss dis xx = 6.7361 | loss dis zz = 1.2945\n",
      "Epoch 221 | time = 0s | loss gen = 29.5030 | loss enc = 74.5737 | loss dis = 9.1095 | loss dis xz = 1.9924 | loss dis xx = 5.7956 | loss dis zz = 1.3215\n",
      "Epoch 222 | time = 0s | loss gen = 31.4450 | loss enc = 80.9296 | loss dis = 9.3770 | loss dis xz = 2.1397 | loss dis xx = 5.8589 | loss dis zz = 1.3785\n",
      "Epoch 223 | time = 0s | loss gen = 31.9899 | loss enc = 77.7902 | loss dis = 8.4926 | loss dis xz = 2.4146 | loss dis xx = 4.6929 | loss dis zz = 1.3851\n",
      "Epoch 224 | time = 0s | loss gen = 29.3274 | loss enc = 76.5980 | loss dis = 10.5248 | loss dis xz = 3.2392 | loss dis xx = 5.9594 | loss dis zz = 1.3262\n",
      "Epoch 225 | time = 0s | loss gen = 29.6005 | loss enc = 81.8352 | loss dis = 10.9598 | loss dis xz = 2.5451 | loss dis xx = 7.1064 | loss dis zz = 1.3083\n",
      "Epoch 226 | time = 0s | loss gen = 28.8182 | loss enc = 83.2947 | loss dis = 9.1672 | loss dis xz = 1.9104 | loss dis xx = 5.9055 | loss dis zz = 1.3514\n",
      "Epoch 227 | time = 0s | loss gen = 29.2908 | loss enc = 79.3732 | loss dis = 8.7039 | loss dis xz = 2.6726 | loss dis xx = 4.7419 | loss dis zz = 1.2895\n",
      "Epoch 228 | time = 0s | loss gen = 30.5072 | loss enc = 81.4826 | loss dis = 9.1268 | loss dis xz = 2.5168 | loss dis xx = 5.3156 | loss dis zz = 1.2944\n",
      "Epoch 229 | time = 0s | loss gen = 30.6449 | loss enc = 78.8700 | loss dis = 8.8101 | loss dis xz = 1.7760 | loss dis xx = 5.7468 | loss dis zz = 1.2873\n",
      "Epoch 230 | time = 0s | loss gen = 31.0263 | loss enc = 81.0524 | loss dis = 9.9958 | loss dis xz = 3.4162 | loss dis xx = 5.2659 | loss dis zz = 1.3136\n",
      "Epoch 231 | time = 0s | loss gen = 32.3577 | loss enc = 76.9503 | loss dis = 9.1461 | loss dis xz = 1.9789 | loss dis xx = 5.8304 | loss dis zz = 1.3367\n",
      "Epoch 232 | time = 0s | loss gen = 30.5181 | loss enc = 80.3625 | loss dis = 9.5276 | loss dis xz = 2.4765 | loss dis xx = 5.7255 | loss dis zz = 1.3256\n",
      "Epoch 233 | time = 0s | loss gen = 31.9066 | loss enc = 81.0571 | loss dis = 9.4895 | loss dis xz = 2.2381 | loss dis xx = 5.9548 | loss dis zz = 1.2965\n",
      "Epoch 234 | time = 0s | loss gen = 30.0568 | loss enc = 79.1312 | loss dis = 8.9108 | loss dis xz = 2.5265 | loss dis xx = 5.0472 | loss dis zz = 1.3371\n",
      "Epoch 235 | time = 0s | loss gen = 28.5754 | loss enc = 81.0519 | loss dis = 8.5171 | loss dis xz = 2.6303 | loss dis xx = 4.5458 | loss dis zz = 1.3410\n",
      "Epoch 236 | time = 0s | loss gen = 30.7381 | loss enc = 83.7781 | loss dis = 9.7177 | loss dis xz = 2.2926 | loss dis xx = 6.0731 | loss dis zz = 1.3520\n",
      "Epoch 237 | time = 0s | loss gen = 31.1749 | loss enc = 84.1075 | loss dis = 9.0297 | loss dis xz = 2.3504 | loss dis xx = 5.3834 | loss dis zz = 1.2958\n",
      "Epoch 238 | time = 0s | loss gen = 30.0697 | loss enc = 83.4839 | loss dis = 10.2938 | loss dis xz = 3.6840 | loss dis xx = 5.3356 | loss dis zz = 1.2743\n",
      "Epoch 239 | time = 0s | loss gen = 30.5108 | loss enc = 84.3905 | loss dis = 8.0598 | loss dis xz = 2.2487 | loss dis xx = 4.5358 | loss dis zz = 1.2753\n",
      "Epoch 240 | time = 0s | loss gen = 29.5844 | loss enc = 88.9790 | loss dis = 9.1183 | loss dis xz = 2.0585 | loss dis xx = 5.7586 | loss dis zz = 1.3012\n",
      "Epoch 241 | time = 0s | loss gen = 30.7322 | loss enc = 87.0203 | loss dis = 9.5986 | loss dis xz = 2.8280 | loss dis xx = 5.4640 | loss dis zz = 1.3066\n",
      "Epoch 242 | time = 0s | loss gen = 30.4961 | loss enc = 83.8178 | loss dis = 9.0687 | loss dis xz = 1.8860 | loss dis xx = 5.9030 | loss dis zz = 1.2797\n",
      "Epoch 243 | time = 0s | loss gen = 29.4737 | loss enc = 89.7785 | loss dis = 10.3586 | loss dis xz = 2.8379 | loss dis xx = 6.1679 | loss dis zz = 1.3528\n",
      "Epoch 244 | time = 0s | loss gen = 29.4532 | loss enc = 84.1186 | loss dis = 9.0457 | loss dis xz = 2.3899 | loss dis xx = 5.3788 | loss dis zz = 1.2770\n",
      "Epoch 245 | time = 0s | loss gen = 30.7798 | loss enc = 88.4593 | loss dis = 8.9419 | loss dis xz = 2.5931 | loss dis xx = 5.0217 | loss dis zz = 1.3271\n",
      "Epoch 246 | time = 0s | loss gen = 29.9550 | loss enc = 92.9139 | loss dis = 7.0572 | loss dis xz = 1.4236 | loss dis xx = 4.3586 | loss dis zz = 1.2750\n",
      "Epoch 247 | time = 0s | loss gen = 28.8469 | loss enc = 92.0714 | loss dis = 8.5964 | loss dis xz = 1.6285 | loss dis xx = 5.7078 | loss dis zz = 1.2600\n",
      "Epoch 248 | time = 0s | loss gen = 29.0363 | loss enc = 92.4980 | loss dis = 9.0022 | loss dis xz = 2.4552 | loss dis xx = 5.2127 | loss dis zz = 1.3344\n",
      "Epoch 249 | time = 0s | loss gen = 30.1431 | loss enc = 87.7294 | loss dis = 7.3417 | loss dis xz = 1.1926 | loss dis xx = 4.8621 | loss dis zz = 1.2870\n",
      "Epoch 250 | time = 0s | loss gen = 27.5802 | loss enc = 91.6088 | loss dis = 8.9840 | loss dis xz = 2.4712 | loss dis xx = 5.2237 | loss dis zz = 1.2891\n",
      "Epoch 251 | time = 0s | loss gen = 27.1069 | loss enc = 84.6823 | loss dis = 7.9686 | loss dis xz = 1.8601 | loss dis xx = 4.7902 | loss dis zz = 1.3183\n",
      "Epoch 252 | time = 0s | loss gen = 27.1612 | loss enc = 92.2091 | loss dis = 9.0708 | loss dis xz = 2.0966 | loss dis xx = 5.6632 | loss dis zz = 1.3111\n",
      "Epoch 253 | time = 0s | loss gen = 26.6931 | loss enc = 82.9433 | loss dis = 9.8096 | loss dis xz = 2.5679 | loss dis xx = 5.9714 | loss dis zz = 1.2703\n",
      "Epoch 254 | time = 0s | loss gen = 24.8613 | loss enc = 85.3329 | loss dis = 8.5840 | loss dis xz = 1.3173 | loss dis xx = 5.9912 | loss dis zz = 1.2755\n",
      "Epoch 255 | time = 0s | loss gen = 27.9664 | loss enc = 95.7271 | loss dis = 9.6597 | loss dis xz = 1.8462 | loss dis xx = 6.5957 | loss dis zz = 1.2178\n",
      "Epoch 256 | time = 0s | loss gen = 26.4364 | loss enc = 85.8965 | loss dis = 10.3003 | loss dis xz = 2.1550 | loss dis xx = 6.8978 | loss dis zz = 1.2474\n",
      "Epoch 257 | time = 0s | loss gen = 26.2943 | loss enc = 90.5887 | loss dis = 7.9997 | loss dis xz = 1.4263 | loss dis xx = 5.3269 | loss dis zz = 1.2464\n",
      "Epoch 258 | time = 0s | loss gen = 24.6033 | loss enc = 84.3881 | loss dis = 9.6821 | loss dis xz = 2.0486 | loss dis xx = 6.3704 | loss dis zz = 1.2631\n",
      "Epoch 259 | time = 0s | loss gen = 23.9885 | loss enc = 87.2327 | loss dis = 10.9688 | loss dis xz = 2.5377 | loss dis xx = 7.1028 | loss dis zz = 1.3283\n",
      "Epoch 260 | time = 0s | loss gen = 27.3087 | loss enc = 87.8030 | loss dis = 9.0889 | loss dis xz = 1.4844 | loss dis xx = 6.3711 | loss dis zz = 1.2334\n",
      "Epoch 261 | time = 0s | loss gen = 25.0633 | loss enc = 89.4657 | loss dis = 8.8196 | loss dis xz = 1.1168 | loss dis xx = 6.4591 | loss dis zz = 1.2437\n",
      "Epoch 262 | time = 0s | loss gen = 25.2404 | loss enc = 85.6242 | loss dis = 10.3548 | loss dis xz = 2.5110 | loss dis xx = 6.5325 | loss dis zz = 1.3113\n",
      "Epoch 263 | time = 0s | loss gen = 26.6141 | loss enc = 82.5144 | loss dis = 9.4903 | loss dis xz = 1.9667 | loss dis xx = 6.3185 | loss dis zz = 1.2051\n",
      "Epoch 264 | time = 0s | loss gen = 24.0390 | loss enc = 86.7509 | loss dis = 7.7833 | loss dis xz = 0.8574 | loss dis xx = 5.7160 | loss dis zz = 1.2099\n",
      "Epoch 265 | time = 0s | loss gen = 25.6677 | loss enc = 85.2557 | loss dis = 10.2092 | loss dis xz = 2.4511 | loss dis xx = 6.5137 | loss dis zz = 1.2444\n",
      "Epoch 266 | time = 0s | loss gen = 25.3200 | loss enc = 87.7107 | loss dis = 9.9944 | loss dis xz = 2.3067 | loss dis xx = 6.4161 | loss dis zz = 1.2715\n",
      "Epoch 267 | time = 0s | loss gen = 24.4641 | loss enc = 79.8808 | loss dis = 9.0413 | loss dis xz = 1.3059 | loss dis xx = 6.4917 | loss dis zz = 1.2437\n",
      "Epoch 268 | time = 0s | loss gen = 25.9052 | loss enc = 81.8093 | loss dis = 8.9937 | loss dis xz = 2.0553 | loss dis xx = 5.6388 | loss dis zz = 1.2997\n",
      "Epoch 269 | time = 0s | loss gen = 25.6481 | loss enc = 92.6839 | loss dis = 9.5839 | loss dis xz = 1.3153 | loss dis xx = 7.0320 | loss dis zz = 1.2365\n",
      "Epoch 270 | time = 0s | loss gen = 25.6112 | loss enc = 89.0109 | loss dis = 9.8234 | loss dis xz = 3.1889 | loss dis xx = 5.4049 | loss dis zz = 1.2296\n",
      "Epoch 271 | time = 0s | loss gen = 24.9817 | loss enc = 84.3857 | loss dis = 8.2020 | loss dis xz = 1.6387 | loss dis xx = 5.2546 | loss dis zz = 1.3087\n",
      "Epoch 272 | time = 0s | loss gen = 24.5291 | loss enc = 86.2070 | loss dis = 9.0497 | loss dis xz = 1.9151 | loss dis xx = 5.9022 | loss dis zz = 1.2324\n",
      "Epoch 273 | time = 0s | loss gen = 25.2651 | loss enc = 86.8688 | loss dis = 8.9290 | loss dis xz = 1.9423 | loss dis xx = 5.7231 | loss dis zz = 1.2636\n",
      "Epoch 274 | time = 0s | loss gen = 25.2093 | loss enc = 86.3949 | loss dis = 10.3087 | loss dis xz = 2.4269 | loss dis xx = 6.5950 | loss dis zz = 1.2869\n",
      "Epoch 275 | time = 0s | loss gen = 24.6763 | loss enc = 83.5375 | loss dis = 8.4077 | loss dis xz = 1.4025 | loss dis xx = 5.7459 | loss dis zz = 1.2594\n",
      "Epoch 276 | time = 0s | loss gen = 24.6657 | loss enc = 81.3028 | loss dis = 9.7780 | loss dis xz = 1.7570 | loss dis xx = 6.7684 | loss dis zz = 1.2526\n",
      "Epoch 277 | time = 0s | loss gen = 24.9539 | loss enc = 83.2465 | loss dis = 8.8816 | loss dis xz = 2.0300 | loss dis xx = 5.6111 | loss dis zz = 1.2406\n",
      "Epoch 278 | time = 0s | loss gen = 23.1029 | loss enc = 86.3894 | loss dis = 9.5650 | loss dis xz = 1.9590 | loss dis xx = 6.3124 | loss dis zz = 1.2936\n",
      "Epoch 279 | time = 0s | loss gen = 24.7956 | loss enc = 83.7527 | loss dis = 9.2196 | loss dis xz = 1.4907 | loss dis xx = 6.5151 | loss dis zz = 1.2138\n",
      "Epoch 280 | time = 0s | loss gen = 25.9307 | loss enc = 87.9202 | loss dis = 8.5560 | loss dis xz = 2.0209 | loss dis xx = 5.2743 | loss dis zz = 1.2609\n",
      "Epoch 281 | time = 0s | loss gen = 22.8758 | loss enc = 85.7377 | loss dis = 10.1023 | loss dis xz = 2.0758 | loss dis xx = 6.7435 | loss dis zz = 1.2830\n",
      "Epoch 282 | time = 0s | loss gen = 24.1635 | loss enc = 83.1605 | loss dis = 8.9349 | loss dis xz = 2.1602 | loss dis xx = 5.4583 | loss dis zz = 1.3164\n",
      "Epoch 283 | time = 0s | loss gen = 22.9628 | loss enc = 83.2130 | loss dis = 8.8657 | loss dis xz = 1.2599 | loss dis xx = 6.3702 | loss dis zz = 1.2355\n",
      "Epoch 284 | time = 0s | loss gen = 22.5207 | loss enc = 79.8836 | loss dis = 10.2381 | loss dis xz = 1.9193 | loss dis xx = 7.0621 | loss dis zz = 1.2567\n",
      "Epoch 285 | time = 0s | loss gen = 23.1856 | loss enc = 82.9558 | loss dis = 9.7294 | loss dis xz = 2.1061 | loss dis xx = 6.3694 | loss dis zz = 1.2539\n",
      "Epoch 286 | time = 0s | loss gen = 24.8293 | loss enc = 79.6687 | loss dis = 9.7841 | loss dis xz = 2.1013 | loss dis xx = 6.4141 | loss dis zz = 1.2688\n",
      "Epoch 287 | time = 0s | loss gen = 23.7647 | loss enc = 83.1956 | loss dis = 8.9456 | loss dis xz = 1.4469 | loss dis xx = 6.2065 | loss dis zz = 1.2922\n",
      "Epoch 288 | time = 0s | loss gen = 21.9952 | loss enc = 77.6916 | loss dis = 10.1486 | loss dis xz = 1.7114 | loss dis xx = 7.1615 | loss dis zz = 1.2757\n",
      "Epoch 289 | time = 0s | loss gen = 22.6700 | loss enc = 80.6433 | loss dis = 9.7326 | loss dis xz = 1.4642 | loss dis xx = 6.9689 | loss dis zz = 1.2995\n",
      "Epoch 290 | time = 0s | loss gen = 22.2270 | loss enc = 80.5662 | loss dis = 10.9682 | loss dis xz = 1.7972 | loss dis xx = 7.8733 | loss dis zz = 1.2977\n",
      "Epoch 291 | time = 0s | loss gen = 22.4320 | loss enc = 82.3316 | loss dis = 11.7945 | loss dis xz = 2.1453 | loss dis xx = 8.4293 | loss dis zz = 1.2198\n",
      "Epoch 292 | time = 0s | loss gen = 21.7004 | loss enc = 78.1819 | loss dis = 8.5520 | loss dis xz = 1.4427 | loss dis xx = 5.8348 | loss dis zz = 1.2745\n",
      "Epoch 293 | time = 0s | loss gen = 21.1055 | loss enc = 74.2302 | loss dis = 9.4422 | loss dis xz = 1.2952 | loss dis xx = 6.8384 | loss dis zz = 1.3087\n",
      "Epoch 294 | time = 0s | loss gen = 20.8623 | loss enc = 72.3762 | loss dis = 8.7632 | loss dis xz = 1.4771 | loss dis xx = 6.0172 | loss dis zz = 1.2689\n",
      "Epoch 295 | time = 0s | loss gen = 21.5196 | loss enc = 77.2382 | loss dis = 10.7176 | loss dis xz = 2.8911 | loss dis xx = 6.5723 | loss dis zz = 1.2542\n",
      "Epoch 296 | time = 0s | loss gen = 21.7928 | loss enc = 74.3264 | loss dis = 10.1451 | loss dis xz = 1.5712 | loss dis xx = 7.2766 | loss dis zz = 1.2973\n",
      "Epoch 297 | time = 0s | loss gen = 20.4034 | loss enc = 69.9143 | loss dis = 9.8211 | loss dis xz = 1.4501 | loss dis xx = 7.0284 | loss dis zz = 1.3425\n",
      "Epoch 298 | time = 0s | loss gen = 20.5529 | loss enc = 68.3982 | loss dis = 9.6161 | loss dis xz = 1.7979 | loss dis xx = 6.5388 | loss dis zz = 1.2795\n",
      "Epoch 299 | time = 0s | loss gen = 20.1640 | loss enc = 67.0183 | loss dis = 10.7786 | loss dis xz = 2.4559 | loss dis xx = 6.9898 | loss dis zz = 1.3328\n",
      "Epoch 300 | time = 0s | loss gen = 19.6994 | loss enc = 68.8552 | loss dis = 11.0553 | loss dis xz = 1.7972 | loss dis xx = 7.9810 | loss dis zz = 1.2771\n",
      "Epoch 301 | time = 0s | loss gen = 19.6646 | loss enc = 69.6232 | loss dis = 11.6501 | loss dis xz = 2.1290 | loss dis xx = 8.2433 | loss dis zz = 1.2778\n",
      "Epoch 302 | time = 0s | loss gen = 18.3793 | loss enc = 66.6201 | loss dis = 11.1507 | loss dis xz = 1.9754 | loss dis xx = 7.8588 | loss dis zz = 1.3165\n",
      "Epoch 303 | time = 0s | loss gen = 19.1648 | loss enc = 66.0538 | loss dis = 10.9795 | loss dis xz = 2.2412 | loss dis xx = 7.4243 | loss dis zz = 1.3140\n",
      "Epoch 304 | time = 0s | loss gen = 18.3778 | loss enc = 68.3491 | loss dis = 10.1124 | loss dis xz = 2.2270 | loss dis xx = 6.5945 | loss dis zz = 1.2908\n",
      "Epoch 305 | time = 0s | loss gen = 18.9617 | loss enc = 68.3901 | loss dis = 11.4296 | loss dis xz = 1.9004 | loss dis xx = 8.1585 | loss dis zz = 1.3708\n",
      "Epoch 306 | time = 0s | loss gen = 19.1362 | loss enc = 70.9330 | loss dis = 10.8446 | loss dis xz = 1.6126 | loss dis xx = 7.9084 | loss dis zz = 1.3237\n",
      "Epoch 307 | time = 0s | loss gen = 16.7213 | loss enc = 62.8165 | loss dis = 11.3945 | loss dis xz = 1.8433 | loss dis xx = 8.2225 | loss dis zz = 1.3287\n",
      "Epoch 308 | time = 0s | loss gen = 18.9020 | loss enc = 64.5483 | loss dis = 10.4564 | loss dis xz = 1.2132 | loss dis xx = 7.8832 | loss dis zz = 1.3600\n",
      "Epoch 309 | time = 0s | loss gen = 17.0403 | loss enc = 67.4985 | loss dis = 11.4731 | loss dis xz = 2.1417 | loss dis xx = 8.0413 | loss dis zz = 1.2902\n",
      "Epoch 310 | time = 0s | loss gen = 18.9978 | loss enc = 65.2208 | loss dis = 10.9083 | loss dis xz = 2.2015 | loss dis xx = 7.3385 | loss dis zz = 1.3684\n",
      "Epoch 311 | time = 0s | loss gen = 18.6458 | loss enc = 63.1870 | loss dis = 9.5008 | loss dis xz = 1.0035 | loss dis xx = 7.1921 | loss dis zz = 1.3051\n",
      "Epoch 312 | time = 0s | loss gen = 16.5825 | loss enc = 58.8972 | loss dis = 11.0192 | loss dis xz = 2.4445 | loss dis xx = 7.2431 | loss dis zz = 1.3316\n",
      "Epoch 313 | time = 0s | loss gen = 18.2545 | loss enc = 64.9910 | loss dis = 11.2387 | loss dis xz = 2.1796 | loss dis xx = 7.7361 | loss dis zz = 1.3230\n",
      "Epoch 314 | time = 0s | loss gen = 17.4560 | loss enc = 61.7600 | loss dis = 10.5356 | loss dis xz = 2.0227 | loss dis xx = 7.2083 | loss dis zz = 1.3047\n",
      "Epoch 315 | time = 0s | loss gen = 17.7405 | loss enc = 64.0561 | loss dis = 9.9566 | loss dis xz = 1.6467 | loss dis xx = 7.0214 | loss dis zz = 1.2885\n",
      "Epoch 316 | time = 0s | loss gen = 17.4099 | loss enc = 60.6403 | loss dis = 10.6471 | loss dis xz = 2.8528 | loss dis xx = 6.4348 | loss dis zz = 1.3594\n",
      "INFO:tensorflow:Saving checkpoint to path train_logs/arrhythmia/alad_snFalse_dzzTrue/dzzenabledTrue//label1/rd2/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoint to path train_logs/arrhythmia/alad_snFalse_dzzTrue/dzzenabledTrue//label1/rd2/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 317 | time = 0s | loss gen = 18.7843 | loss enc = 63.7288 | loss dis = 9.9914 | loss dis xz = 1.2826 | loss dis xx = 7.4026 | loss dis zz = 1.3062\n",
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Tensor' object has no attribute 'to_proto'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Tensor' object has no attribute 'to_proto'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 318 | time = 0s | loss gen = 17.5651 | loss enc = 54.6366 | loss dis = 11.2205 | loss dis xz = 1.4433 | loss dis xx = 8.4225 | loss dis zz = 1.3547\n",
      "Epoch 319 | time = 0s | loss gen = 18.0104 | loss enc = 64.9005 | loss dis = 10.5437 | loss dis xz = 1.8258 | loss dis xx = 7.3956 | loss dis zz = 1.3223\n",
      "Epoch 320 | time = 0s | loss gen = 17.5269 | loss enc = 63.6594 | loss dis = 11.4522 | loss dis xz = 2.4538 | loss dis xx = 7.6607 | loss dis zz = 1.3377\n",
      "Epoch 321 | time = 0s | loss gen = 16.9228 | loss enc = 59.9760 | loss dis = 10.5096 | loss dis xz = 2.0427 | loss dis xx = 7.1585 | loss dis zz = 1.3083\n",
      "Epoch 322 | time = 0s | loss gen = 19.4435 | loss enc = 62.5744 | loss dis = 10.3700 | loss dis xz = 1.7700 | loss dis xx = 7.2487 | loss dis zz = 1.3513\n",
      "Epoch 323 | time = 0s | loss gen = 17.5806 | loss enc = 63.4479 | loss dis = 9.3715 | loss dis xz = 1.6860 | loss dis xx = 6.4141 | loss dis zz = 1.2714\n",
      "Epoch 324 | time = 0s | loss gen = 18.4000 | loss enc = 66.5461 | loss dis = 9.0857 | loss dis xz = 1.3422 | loss dis xx = 6.4222 | loss dis zz = 1.3212\n",
      "Epoch 325 | time = 0s | loss gen = 17.8630 | loss enc = 64.0308 | loss dis = 9.7108 | loss dis xz = 2.5180 | loss dis xx = 5.8948 | loss dis zz = 1.2980\n",
      "Epoch 326 | time = 0s | loss gen = 17.9367 | loss enc = 66.3161 | loss dis = 9.5502 | loss dis xz = 1.5580 | loss dis xx = 6.6213 | loss dis zz = 1.3710\n",
      "Epoch 327 | time = 0s | loss gen = 17.9006 | loss enc = 68.0084 | loss dis = 9.3698 | loss dis xz = 1.6493 | loss dis xx = 6.3122 | loss dis zz = 1.4083\n",
      "Epoch 328 | time = 0s | loss gen = 18.3865 | loss enc = 63.5928 | loss dis = 9.7953 | loss dis xz = 1.7445 | loss dis xx = 6.6787 | loss dis zz = 1.3721\n",
      "Epoch 329 | time = 0s | loss gen = 17.3222 | loss enc = 64.8360 | loss dis = 8.7151 | loss dis xz = 1.3570 | loss dis xx = 5.9889 | loss dis zz = 1.3692\n",
      "Epoch 330 | time = 0s | loss gen = 18.3543 | loss enc = 64.2545 | loss dis = 8.9003 | loss dis xz = 1.5562 | loss dis xx = 5.9750 | loss dis zz = 1.3691\n",
      "Epoch 331 | time = 0s | loss gen = 17.8511 | loss enc = 59.1843 | loss dis = 9.2030 | loss dis xz = 2.3353 | loss dis xx = 5.5241 | loss dis zz = 1.3436\n",
      "Epoch 332 | time = 0s | loss gen = 19.1146 | loss enc = 63.8394 | loss dis = 10.3560 | loss dis xz = 1.9620 | loss dis xx = 7.0239 | loss dis zz = 1.3700\n",
      "Epoch 333 | time = 0s | loss gen = 17.0706 | loss enc = 59.7756 | loss dis = 8.9635 | loss dis xz = 2.3038 | loss dis xx = 5.3340 | loss dis zz = 1.3257\n",
      "Epoch 334 | time = 0s | loss gen = 17.4727 | loss enc = 57.8725 | loss dis = 8.6275 | loss dis xz = 1.6472 | loss dis xx = 5.6519 | loss dis zz = 1.3284\n",
      "Epoch 335 | time = 0s | loss gen = 18.9016 | loss enc = 63.8238 | loss dis = 10.6503 | loss dis xz = 2.5558 | loss dis xx = 6.8049 | loss dis zz = 1.2896\n",
      "Epoch 336 | time = 0s | loss gen = 17.3722 | loss enc = 62.7663 | loss dis = 8.8268 | loss dis xz = 1.7240 | loss dis xx = 5.7977 | loss dis zz = 1.3051\n",
      "Epoch 337 | time = 0s | loss gen = 18.5992 | loss enc = 63.7688 | loss dis = 8.9486 | loss dis xz = 1.9047 | loss dis xx = 5.7270 | loss dis zz = 1.3170\n",
      "Epoch 338 | time = 0s | loss gen = 17.9603 | loss enc = 61.0639 | loss dis = 9.7156 | loss dis xz = 3.0243 | loss dis xx = 5.3485 | loss dis zz = 1.3428\n",
      "Epoch 339 | time = 0s | loss gen = 19.8004 | loss enc = 63.9464 | loss dis = 8.3931 | loss dis xz = 2.0501 | loss dis xx = 4.9681 | loss dis zz = 1.3748\n",
      "Epoch 340 | time = 0s | loss gen = 18.0855 | loss enc = 58.2586 | loss dis = 7.5370 | loss dis xz = 1.4202 | loss dis xx = 4.8152 | loss dis zz = 1.3016\n",
      "Epoch 341 | time = 0s | loss gen = 17.2642 | loss enc = 58.3293 | loss dis = 8.5349 | loss dis xz = 1.6775 | loss dis xx = 5.5411 | loss dis zz = 1.3163\n",
      "Epoch 342 | time = 0s | loss gen = 17.9377 | loss enc = 63.6427 | loss dis = 8.6501 | loss dis xz = 2.0664 | loss dis xx = 5.1912 | loss dis zz = 1.3925\n",
      "Epoch 343 | time = 0s | loss gen = 18.4382 | loss enc = 61.5766 | loss dis = 8.9611 | loss dis xz = 2.3001 | loss dis xx = 5.3229 | loss dis zz = 1.3380\n",
      "Epoch 344 | time = 0s | loss gen = 18.7120 | loss enc = 57.1193 | loss dis = 7.7613 | loss dis xz = 1.9383 | loss dis xx = 4.4513 | loss dis zz = 1.3716\n",
      "Epoch 345 | time = 0s | loss gen = 17.4985 | loss enc = 59.3203 | loss dis = 7.9688 | loss dis xz = 1.8277 | loss dis xx = 4.8116 | loss dis zz = 1.3296\n",
      "Epoch 346 | time = 0s | loss gen = 18.6079 | loss enc = 57.7423 | loss dis = 8.7274 | loss dis xz = 1.8164 | loss dis xx = 5.5360 | loss dis zz = 1.3750\n",
      "Epoch 347 | time = 0s | loss gen = 18.2341 | loss enc = 60.7239 | loss dis = 8.6306 | loss dis xz = 1.6002 | loss dis xx = 5.6782 | loss dis zz = 1.3522\n",
      "Epoch 348 | time = 0s | loss gen = 17.9929 | loss enc = 62.5409 | loss dis = 8.1525 | loss dis xz = 1.3076 | loss dis xx = 5.5160 | loss dis zz = 1.3289\n",
      "Epoch 349 | time = 0s | loss gen = 18.5733 | loss enc = 58.6021 | loss dis = 8.7970 | loss dis xz = 1.7053 | loss dis xx = 5.6851 | loss dis zz = 1.4066\n",
      "Epoch 350 | time = 0s | loss gen = 17.5054 | loss enc = 59.3529 | loss dis = 8.6694 | loss dis xz = 1.6634 | loss dis xx = 5.6552 | loss dis zz = 1.3508\n",
      "Epoch 351 | time = 0s | loss gen = 18.7967 | loss enc = 61.7688 | loss dis = 9.0721 | loss dis xz = 1.8234 | loss dis xx = 5.8803 | loss dis zz = 1.3684\n",
      "Epoch 352 | time = 0s | loss gen = 17.3332 | loss enc = 59.6054 | loss dis = 8.2692 | loss dis xz = 1.4755 | loss dis xx = 5.4288 | loss dis zz = 1.3649\n",
      "Epoch 353 | time = 0s | loss gen = 17.6539 | loss enc = 59.7760 | loss dis = 7.7777 | loss dis xz = 1.8933 | loss dis xx = 4.4712 | loss dis zz = 1.4131\n",
      "Epoch 354 | time = 0s | loss gen = 16.5955 | loss enc = 56.4222 | loss dis = 8.6568 | loss dis xz = 1.6539 | loss dis xx = 5.6251 | loss dis zz = 1.3778\n",
      "Epoch 355 | time = 0s | loss gen = 18.4061 | loss enc = 58.8604 | loss dis = 8.6620 | loss dis xz = 1.4071 | loss dis xx = 5.9222 | loss dis zz = 1.3327\n",
      "Epoch 356 | time = 0s | loss gen = 18.8736 | loss enc = 55.7589 | loss dis = 8.0126 | loss dis xz = 2.0948 | loss dis xx = 4.5721 | loss dis zz = 1.3456\n",
      "Epoch 357 | time = 0s | loss gen = 18.1138 | loss enc = 55.9886 | loss dis = 8.5145 | loss dis xz = 2.1757 | loss dis xx = 4.9836 | loss dis zz = 1.3551\n",
      "Epoch 358 | time = 0s | loss gen = 18.0535 | loss enc = 56.9056 | loss dis = 8.4008 | loss dis xz = 2.3061 | loss dis xx = 4.8104 | loss dis zz = 1.2843\n",
      "Epoch 359 | time = 0s | loss gen = 19.1089 | loss enc = 60.6319 | loss dis = 7.3575 | loss dis xz = 1.5315 | loss dis xx = 4.4656 | loss dis zz = 1.3605\n",
      "Epoch 360 | time = 0s | loss gen = 18.0081 | loss enc = 52.7419 | loss dis = 9.2107 | loss dis xz = 2.3771 | loss dis xx = 5.4875 | loss dis zz = 1.3461\n",
      "Epoch 361 | time = 0s | loss gen = 18.8228 | loss enc = 56.3939 | loss dis = 8.8451 | loss dis xz = 2.0378 | loss dis xx = 5.5215 | loss dis zz = 1.2858\n",
      "Epoch 362 | time = 0s | loss gen = 17.8316 | loss enc = 56.5142 | loss dis = 7.4650 | loss dis xz = 1.6249 | loss dis xx = 4.5154 | loss dis zz = 1.3247\n",
      "Epoch 363 | time = 0s | loss gen = 17.8784 | loss enc = 59.5826 | loss dis = 7.8585 | loss dis xz = 1.7885 | loss dis xx = 4.6988 | loss dis zz = 1.3712\n",
      "Epoch 364 | time = 0s | loss gen = 18.0970 | loss enc = 57.3164 | loss dis = 7.9631 | loss dis xz = 1.5856 | loss dis xx = 5.0571 | loss dis zz = 1.3204\n",
      "Epoch 365 | time = 0s | loss gen = 17.2696 | loss enc = 54.2089 | loss dis = 7.0753 | loss dis xz = 1.8287 | loss dis xx = 3.9312 | loss dis zz = 1.3154\n",
      "Epoch 366 | time = 0s | loss gen = 17.9621 | loss enc = 54.4465 | loss dis = 6.9717 | loss dis xz = 1.0386 | loss dis xx = 4.5307 | loss dis zz = 1.4024\n",
      "Epoch 367 | time = 0s | loss gen = 18.3558 | loss enc = 53.6939 | loss dis = 7.5098 | loss dis xz = 1.2287 | loss dis xx = 4.8832 | loss dis zz = 1.3980\n",
      "Epoch 368 | time = 0s | loss gen = 18.0088 | loss enc = 55.8670 | loss dis = 9.1969 | loss dis xz = 2.9240 | loss dis xx = 4.9127 | loss dis zz = 1.3602\n",
      "Epoch 369 | time = 0s | loss gen = 18.0758 | loss enc = 56.5550 | loss dis = 7.2979 | loss dis xz = 1.5844 | loss dis xx = 4.3399 | loss dis zz = 1.3737\n",
      "Epoch 370 | time = 0s | loss gen = 18.1164 | loss enc = 58.0785 | loss dis = 7.0663 | loss dis xz = 1.5025 | loss dis xx = 4.2407 | loss dis zz = 1.3231\n",
      "Epoch 371 | time = 0s | loss gen = 17.3946 | loss enc = 54.2384 | loss dis = 8.7649 | loss dis xz = 2.8674 | loss dis xx = 4.5169 | loss dis zz = 1.3806\n",
      "Epoch 372 | time = 0s | loss gen = 16.7299 | loss enc = 53.3042 | loss dis = 8.9380 | loss dis xz = 2.3198 | loss dis xx = 5.2195 | loss dis zz = 1.3987\n",
      "Epoch 373 | time = 0s | loss gen = 17.9023 | loss enc = 57.7976 | loss dis = 7.2464 | loss dis xz = 1.3948 | loss dis xx = 4.4781 | loss dis zz = 1.3736\n",
      "Epoch 374 | time = 0s | loss gen = 16.8646 | loss enc = 54.9725 | loss dis = 7.2322 | loss dis xz = 1.7075 | loss dis xx = 4.1268 | loss dis zz = 1.3980\n",
      "Epoch 375 | time = 0s | loss gen = 17.4513 | loss enc = 54.5528 | loss dis = 8.0171 | loss dis xz = 2.4120 | loss dis xx = 4.2139 | loss dis zz = 1.3913\n",
      "Epoch 376 | time = 0s | loss gen = 17.2023 | loss enc = 54.5520 | loss dis = 6.6865 | loss dis xz = 1.3554 | loss dis xx = 3.9504 | loss dis zz = 1.3807\n",
      "Epoch 377 | time = 0s | loss gen = 17.5991 | loss enc = 55.4339 | loss dis = 6.8211 | loss dis xz = 1.5268 | loss dis xx = 3.8685 | loss dis zz = 1.4258\n",
      "Epoch 378 | time = 0s | loss gen = 16.2232 | loss enc = 55.8629 | loss dis = 7.5438 | loss dis xz = 1.8901 | loss dis xx = 4.2631 | loss dis zz = 1.3907\n",
      "Epoch 379 | time = 0s | loss gen = 16.5759 | loss enc = 51.2559 | loss dis = 6.6886 | loss dis xz = 1.3573 | loss dis xx = 4.0003 | loss dis zz = 1.3310\n",
      "Epoch 380 | time = 0s | loss gen = 17.2290 | loss enc = 54.6460 | loss dis = 7.9563 | loss dis xz = 1.5844 | loss dis xx = 4.9053 | loss dis zz = 1.4666\n",
      "Epoch 381 | time = 0s | loss gen = 17.5867 | loss enc = 55.9494 | loss dis = 7.5615 | loss dis xz = 1.7831 | loss dis xx = 4.4221 | loss dis zz = 1.3563\n",
      "Epoch 382 | time = 0s | loss gen = 17.4329 | loss enc = 57.3976 | loss dis = 7.1691 | loss dis xz = 1.5547 | loss dis xx = 4.1967 | loss dis zz = 1.4176\n",
      "Epoch 383 | time = 0s | loss gen = 16.9787 | loss enc = 56.0021 | loss dis = 7.2330 | loss dis xz = 1.5335 | loss dis xx = 4.2710 | loss dis zz = 1.4285\n",
      "Epoch 384 | time = 0s | loss gen = 15.8063 | loss enc = 56.6366 | loss dis = 6.9107 | loss dis xz = 1.1027 | loss dis xx = 4.4724 | loss dis zz = 1.3356\n",
      "Epoch 385 | time = 0s | loss gen = 16.7400 | loss enc = 58.1112 | loss dis = 6.5736 | loss dis xz = 1.3128 | loss dis xx = 3.8572 | loss dis zz = 1.4036\n",
      "Epoch 386 | time = 0s | loss gen = 16.7768 | loss enc = 54.5454 | loss dis = 7.7581 | loss dis xz = 1.8885 | loss dis xx = 4.4978 | loss dis zz = 1.3717\n",
      "Epoch 387 | time = 0s | loss gen = 15.9626 | loss enc = 55.2190 | loss dis = 6.9631 | loss dis xz = 1.4147 | loss dis xx = 4.1502 | loss dis zz = 1.3982\n",
      "Epoch 388 | time = 0s | loss gen = 17.3642 | loss enc = 57.8134 | loss dis = 6.6425 | loss dis xz = 1.3291 | loss dis xx = 3.9920 | loss dis zz = 1.3214\n",
      "Epoch 389 | time = 0s | loss gen = 15.5249 | loss enc = 55.2659 | loss dis = 7.0008 | loss dis xz = 0.9808 | loss dis xx = 4.6143 | loss dis zz = 1.4057\n",
      "Epoch 390 | time = 0s | loss gen = 16.8795 | loss enc = 55.3392 | loss dis = 7.0009 | loss dis xz = 1.5333 | loss dis xx = 4.1048 | loss dis zz = 1.3627\n",
      "Epoch 391 | time = 0s | loss gen = 16.6556 | loss enc = 57.0450 | loss dis = 6.2284 | loss dis xz = 1.1724 | loss dis xx = 3.7169 | loss dis zz = 1.3391\n",
      "Epoch 392 | time = 0s | loss gen = 16.9954 | loss enc = 55.8297 | loss dis = 7.0392 | loss dis xz = 1.5396 | loss dis xx = 4.1228 | loss dis zz = 1.3769\n",
      "Epoch 393 | time = 0s | loss gen = 16.5036 | loss enc = 61.5642 | loss dis = 6.5915 | loss dis xz = 1.2921 | loss dis xx = 3.9510 | loss dis zz = 1.3484\n",
      "Epoch 394 | time = 0s | loss gen = 17.5118 | loss enc = 55.7505 | loss dis = 7.6517 | loss dis xz = 1.6183 | loss dis xx = 4.6648 | loss dis zz = 1.3686\n",
      "Epoch 395 | time = 0s | loss gen = 16.6286 | loss enc = 55.3966 | loss dis = 6.8362 | loss dis xz = 1.5752 | loss dis xx = 3.8220 | loss dis zz = 1.4391\n",
      "Epoch 396 | time = 0s | loss gen = 16.0424 | loss enc = 54.7222 | loss dis = 7.3855 | loss dis xz = 1.3508 | loss dis xx = 4.7117 | loss dis zz = 1.3229\n",
      "Epoch 397 | time = 0s | loss gen = 15.6715 | loss enc = 59.8203 | loss dis = 6.3868 | loss dis xz = 1.0501 | loss dis xx = 4.0220 | loss dis zz = 1.3147\n",
      "Epoch 398 | time = 0s | loss gen = 16.4597 | loss enc = 57.3613 | loss dis = 7.1984 | loss dis xz = 1.4203 | loss dis xx = 4.4047 | loss dis zz = 1.3734\n",
      "Epoch 399 | time = 0s | loss gen = 17.2885 | loss enc = 58.7017 | loss dis = 6.4757 | loss dis xz = 0.8593 | loss dis xx = 4.2822 | loss dis zz = 1.3343\n",
      "Epoch 400 | time = 0s | loss gen = 15.6043 | loss enc = 56.1722 | loss dis = 6.8390 | loss dis xz = 1.4235 | loss dis xx = 4.0823 | loss dis zz = 1.3333\n",
      "Epoch 401 | time = 0s | loss gen = 15.0624 | loss enc = 57.5805 | loss dis = 7.6064 | loss dis xz = 1.6958 | loss dis xx = 4.4676 | loss dis zz = 1.4430\n",
      "Epoch 402 | time = 0s | loss gen = 16.7863 | loss enc = 60.4921 | loss dis = 7.6816 | loss dis xz = 1.1741 | loss dis xx = 5.0995 | loss dis zz = 1.4080\n",
      "Epoch 403 | time = 0s | loss gen = 15.8045 | loss enc = 56.9544 | loss dis = 6.5839 | loss dis xz = 0.7541 | loss dis xx = 4.4708 | loss dis zz = 1.3590\n",
      "Epoch 404 | time = 0s | loss gen = 15.7785 | loss enc = 54.2240 | loss dis = 7.1398 | loss dis xz = 0.8621 | loss dis xx = 4.9350 | loss dis zz = 1.3427\n",
      "Epoch 405 | time = 0s | loss gen = 15.1782 | loss enc = 54.6122 | loss dis = 7.6235 | loss dis xz = 1.1856 | loss dis xx = 5.0301 | loss dis zz = 1.4077\n",
      "Epoch 406 | time = 0s | loss gen = 15.8313 | loss enc = 57.4006 | loss dis = 6.8524 | loss dis xz = 1.0512 | loss dis xx = 4.3497 | loss dis zz = 1.4516\n",
      "Epoch 407 | time = 0s | loss gen = 15.0820 | loss enc = 57.9117 | loss dis = 7.0462 | loss dis xz = 1.5242 | loss dis xx = 4.1729 | loss dis zz = 1.3491\n",
      "Epoch 408 | time = 0s | loss gen = 16.5511 | loss enc = 60.6610 | loss dis = 7.2787 | loss dis xz = 1.3786 | loss dis xx = 4.5339 | loss dis zz = 1.3662\n",
      "Epoch 409 | time = 0s | loss gen = 15.6644 | loss enc = 54.7969 | loss dis = 7.0023 | loss dis xz = 0.9676 | loss dis xx = 4.6316 | loss dis zz = 1.4030\n",
      "Epoch 410 | time = 0s | loss gen = 15.0094 | loss enc = 53.7939 | loss dis = 6.4290 | loss dis xz = 0.6844 | loss dis xx = 4.3373 | loss dis zz = 1.4073\n",
      "Epoch 411 | time = 0s | loss gen = 14.7390 | loss enc = 58.5495 | loss dis = 7.0357 | loss dis xz = 0.7720 | loss dis xx = 4.9213 | loss dis zz = 1.3424\n",
      "Epoch 412 | time = 0s | loss gen = 16.4642 | loss enc = 59.3870 | loss dis = 6.2810 | loss dis xz = 0.8236 | loss dis xx = 4.0891 | loss dis zz = 1.3683\n",
      "Epoch 413 | time = 0s | loss gen = 15.1915 | loss enc = 53.7667 | loss dis = 5.9836 | loss dis xz = 1.0385 | loss dis xx = 3.5570 | loss dis zz = 1.3880\n",
      "Epoch 414 | time = 0s | loss gen = 14.7853 | loss enc = 55.4302 | loss dis = 6.6654 | loss dis xz = 1.0387 | loss dis xx = 4.1921 | loss dis zz = 1.4346\n",
      "Epoch 415 | time = 0s | loss gen = 14.4453 | loss enc = 55.5554 | loss dis = 7.0215 | loss dis xz = 1.0788 | loss dis xx = 4.5316 | loss dis zz = 1.4111\n",
      "Epoch 416 | time = 0s | loss gen = 14.9811 | loss enc = 57.7320 | loss dis = 6.8568 | loss dis xz = 0.8178 | loss dis xx = 4.6546 | loss dis zz = 1.3844\n",
      "Epoch 417 | time = 0s | loss gen = 14.3635 | loss enc = 54.5195 | loss dis = 6.8847 | loss dis xz = 1.0289 | loss dis xx = 4.4950 | loss dis zz = 1.3608\n",
      "Epoch 418 | time = 0s | loss gen = 15.1448 | loss enc = 56.6494 | loss dis = 7.1647 | loss dis xz = 1.1109 | loss dis xx = 4.6730 | loss dis zz = 1.3809\n",
      "Epoch 419 | time = 0s | loss gen = 14.9759 | loss enc = 55.3921 | loss dis = 6.7638 | loss dis xz = 0.4540 | loss dis xx = 4.9265 | loss dis zz = 1.3833\n",
      "Epoch 420 | time = 0s | loss gen = 14.4345 | loss enc = 57.7284 | loss dis = 7.7204 | loss dis xz = 1.2976 | loss dis xx = 5.0428 | loss dis zz = 1.3800\n",
      "Epoch 421 | time = 0s | loss gen = 14.5998 | loss enc = 51.9320 | loss dis = 7.9291 | loss dis xz = 0.8793 | loss dis xx = 5.6960 | loss dis zz = 1.3538\n",
      "Epoch 422 | time = 0s | loss gen = 13.9131 | loss enc = 55.7349 | loss dis = 6.9807 | loss dis xz = 0.7983 | loss dis xx = 4.7196 | loss dis zz = 1.4629\n",
      "Epoch 423 | time = 0s | loss gen = 15.3287 | loss enc = 55.4882 | loss dis = 7.7708 | loss dis xz = 0.8522 | loss dis xx = 5.5413 | loss dis zz = 1.3773\n",
      "Epoch 424 | time = 0s | loss gen = 14.6949 | loss enc = 56.4408 | loss dis = 7.6383 | loss dis xz = 0.7742 | loss dis xx = 5.4351 | loss dis zz = 1.4290\n",
      "Epoch 425 | time = 0s | loss gen = 13.6184 | loss enc = 51.5555 | loss dis = 6.7352 | loss dis xz = 0.8852 | loss dis xx = 4.4244 | loss dis zz = 1.4256\n",
      "Epoch 426 | time = 0s | loss gen = 13.3099 | loss enc = 53.3237 | loss dis = 7.6735 | loss dis xz = 0.6120 | loss dis xx = 5.6481 | loss dis zz = 1.4135\n",
      "INFO:tensorflow:Saving checkpoint to path train_logs/arrhythmia/alad_snFalse_dzzTrue/dzzenabledTrue//label1/rd2/model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoint to path train_logs/arrhythmia/alad_snFalse_dzzTrue/dzzenabledTrue//label1/rd2/model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427 | time = 0s | loss gen = 13.4905 | loss enc = 48.4543 | loss dis = 7.6052 | loss dis xz = 0.7868 | loss dis xx = 5.3475 | loss dis zz = 1.4710\n",
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Tensor' object has no attribute 'to_proto'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Tensor' object has no attribute 'to_proto'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 428 | time = 0s | loss gen = 13.8101 | loss enc = 52.6160 | loss dis = 7.8015 | loss dis xz = 0.5234 | loss dis xx = 5.8684 | loss dis zz = 1.4096\n",
      "Epoch 429 | time = 0s | loss gen = 14.5573 | loss enc = 55.2568 | loss dis = 7.1971 | loss dis xz = 0.6141 | loss dis xx = 5.2217 | loss dis zz = 1.3612\n",
      "Epoch 430 | time = 0s | loss gen = 13.2037 | loss enc = 48.9417 | loss dis = 7.2214 | loss dis xz = 0.5674 | loss dis xx = 5.2124 | loss dis zz = 1.4416\n",
      "Epoch 431 | time = 0s | loss gen = 13.4589 | loss enc = 54.2859 | loss dis = 7.7790 | loss dis xz = 0.6704 | loss dis xx = 5.7062 | loss dis zz = 1.4025\n",
      "Epoch 432 | time = 0s | loss gen = 13.4148 | loss enc = 51.6441 | loss dis = 7.4419 | loss dis xz = 0.8718 | loss dis xx = 5.1510 | loss dis zz = 1.4191\n",
      "Epoch 433 | time = 0s | loss gen = 12.8054 | loss enc = 49.7071 | loss dis = 7.5211 | loss dis xz = 0.7022 | loss dis xx = 5.4047 | loss dis zz = 1.4143\n",
      "Epoch 434 | time = 0s | loss gen = 13.1911 | loss enc = 51.4730 | loss dis = 7.4498 | loss dis xz = 0.7011 | loss dis xx = 5.3236 | loss dis zz = 1.4252\n",
      "Epoch 435 | time = 0s | loss gen = 13.0514 | loss enc = 50.9301 | loss dis = 8.2241 | loss dis xz = 0.9420 | loss dis xx = 5.8365 | loss dis zz = 1.4457\n",
      "Epoch 436 | time = 0s | loss gen = 12.6077 | loss enc = 49.3826 | loss dis = 6.9370 | loss dis xz = 0.5330 | loss dis xx = 4.9549 | loss dis zz = 1.4491\n",
      "Epoch 437 | time = 0s | loss gen = 13.5041 | loss enc = 51.3556 | loss dis = 7.3498 | loss dis xz = 0.6451 | loss dis xx = 5.3079 | loss dis zz = 1.3969\n",
      "Epoch 438 | time = 0s | loss gen = 12.6956 | loss enc = 46.0276 | loss dis = 7.7659 | loss dis xz = 0.6916 | loss dis xx = 5.6705 | loss dis zz = 1.4038\n",
      "Epoch 439 | time = 0s | loss gen = 13.5806 | loss enc = 50.6575 | loss dis = 7.0244 | loss dis xz = 0.7664 | loss dis xx = 4.8045 | loss dis zz = 1.4535\n",
      "Epoch 440 | time = 0s | loss gen = 13.4857 | loss enc = 50.6386 | loss dis = 7.3131 | loss dis xz = 0.9862 | loss dis xx = 4.8970 | loss dis zz = 1.4299\n",
      "Epoch 441 | time = 0s | loss gen = 12.5709 | loss enc = 50.5368 | loss dis = 7.6533 | loss dis xz = 1.0129 | loss dis xx = 5.1804 | loss dis zz = 1.4600\n",
      "Epoch 442 | time = 0s | loss gen = 13.0081 | loss enc = 47.4936 | loss dis = 7.6258 | loss dis xz = 0.5722 | loss dis xx = 5.6112 | loss dis zz = 1.4424\n",
      "Epoch 443 | time = 0s | loss gen = 13.0253 | loss enc = 46.9396 | loss dis = 7.8771 | loss dis xz = 1.0414 | loss dis xx = 5.3788 | loss dis zz = 1.4569\n",
      "Epoch 444 | time = 0s | loss gen = 13.4520 | loss enc = 46.0557 | loss dis = 7.1383 | loss dis xz = 0.5622 | loss dis xx = 5.1301 | loss dis zz = 1.4460\n",
      "Epoch 445 | time = 0s | loss gen = 13.8136 | loss enc = 49.6674 | loss dis = 7.6419 | loss dis xz = 0.6582 | loss dis xx = 5.5224 | loss dis zz = 1.4612\n",
      "Epoch 446 | time = 0s | loss gen = 13.4588 | loss enc = 44.2276 | loss dis = 7.2232 | loss dis xz = 0.9343 | loss dis xx = 4.8598 | loss dis zz = 1.4292\n",
      "Epoch 447 | time = 0s | loss gen = 12.9226 | loss enc = 46.9104 | loss dis = 7.4378 | loss dis xz = 0.5051 | loss dis xx = 5.4900 | loss dis zz = 1.4428\n",
      "Epoch 448 | time = 0s | loss gen = 13.4153 | loss enc = 44.3794 | loss dis = 7.5455 | loss dis xz = 0.7595 | loss dis xx = 5.2861 | loss dis zz = 1.4999\n",
      "Epoch 449 | time = 0s | loss gen = 12.6817 | loss enc = 48.0568 | loss dis = 8.1027 | loss dis xz = 1.0042 | loss dis xx = 5.6683 | loss dis zz = 1.4302\n",
      "Epoch 450 | time = 0s | loss gen = 13.2537 | loss enc = 45.4342 | loss dis = 7.6420 | loss dis xz = 0.8767 | loss dis xx = 5.3029 | loss dis zz = 1.4624\n",
      "Epoch 451 | time = 0s | loss gen = 12.9928 | loss enc = 45.4627 | loss dis = 7.0485 | loss dis xz = 0.9700 | loss dis xx = 4.5294 | loss dis zz = 1.5491\n",
      "Epoch 452 | time = 0s | loss gen = 13.4030 | loss enc = 45.6437 | loss dis = 7.5449 | loss dis xz = 1.3233 | loss dis xx = 4.7661 | loss dis zz = 1.4555\n",
      "Epoch 453 | time = 0s | loss gen = 13.7990 | loss enc = 43.9217 | loss dis = 6.8482 | loss dis xz = 0.9162 | loss dis xx = 4.4931 | loss dis zz = 1.4389\n",
      "Epoch 454 | time = 0s | loss gen = 13.6138 | loss enc = 47.9228 | loss dis = 7.1170 | loss dis xz = 1.1353 | loss dis xx = 4.5247 | loss dis zz = 1.4571\n",
      "Epoch 455 | time = 0s | loss gen = 13.4058 | loss enc = 43.1732 | loss dis = 7.0214 | loss dis xz = 0.6635 | loss dis xx = 4.8954 | loss dis zz = 1.4625\n",
      "Epoch 456 | time = 0s | loss gen = 13.7146 | loss enc = 44.5416 | loss dis = 7.6126 | loss dis xz = 0.8670 | loss dis xx = 5.2527 | loss dis zz = 1.4929\n",
      "Epoch 457 | time = 0s | loss gen = 13.2453 | loss enc = 42.5881 | loss dis = 7.3641 | loss dis xz = 0.7292 | loss dis xx = 5.1426 | loss dis zz = 1.4923\n",
      "Epoch 458 | time = 0s | loss gen = 12.4308 | loss enc = 47.4033 | loss dis = 6.2694 | loss dis xz = 0.6355 | loss dis xx = 4.1372 | loss dis zz = 1.4967\n",
      "Epoch 459 | time = 0s | loss gen = 13.1140 | loss enc = 44.9022 | loss dis = 8.5820 | loss dis xz = 1.4704 | loss dis xx = 5.6639 | loss dis zz = 1.4477\n",
      "Epoch 460 | time = 0s | loss gen = 13.0853 | loss enc = 47.0590 | loss dis = 7.0853 | loss dis xz = 0.8775 | loss dis xx = 4.7443 | loss dis zz = 1.4634\n",
      "Epoch 461 | time = 0s | loss gen = 13.3582 | loss enc = 46.7439 | loss dis = 6.5156 | loss dis xz = 0.3717 | loss dis xx = 4.6413 | loss dis zz = 1.5026\n",
      "Epoch 462 | time = 0s | loss gen = 13.5217 | loss enc = 44.8415 | loss dis = 6.6955 | loss dis xz = 0.8205 | loss dis xx = 4.4020 | loss dis zz = 1.4729\n",
      "Epoch 463 | time = 0s | loss gen = 14.3537 | loss enc = 45.3831 | loss dis = 6.6204 | loss dis xz = 0.5184 | loss dis xx = 4.5787 | loss dis zz = 1.5232\n",
      "Epoch 464 | time = 0s | loss gen = 14.2644 | loss enc = 46.1294 | loss dis = 8.0983 | loss dis xz = 1.1947 | loss dis xx = 5.3430 | loss dis zz = 1.5606\n",
      "Epoch 465 | time = 0s | loss gen = 12.5938 | loss enc = 44.9826 | loss dis = 7.1883 | loss dis xz = 0.8124 | loss dis xx = 4.9222 | loss dis zz = 1.4537\n",
      "Epoch 466 | time = 0s | loss gen = 13.4258 | loss enc = 45.0066 | loss dis = 6.3672 | loss dis xz = 0.7619 | loss dis xx = 4.2153 | loss dis zz = 1.3899\n",
      "Epoch 467 | time = 0s | loss gen = 13.7923 | loss enc = 46.6688 | loss dis = 6.9152 | loss dis xz = 0.6631 | loss dis xx = 4.7872 | loss dis zz = 1.4648\n",
      "Epoch 468 | time = 0s | loss gen = 14.2704 | loss enc = 47.7000 | loss dis = 6.8628 | loss dis xz = 0.8793 | loss dis xx = 4.5562 | loss dis zz = 1.4273\n",
      "Epoch 469 | time = 0s | loss gen = 12.6905 | loss enc = 42.8197 | loss dis = 7.1401 | loss dis xz = 0.5894 | loss dis xx = 5.0126 | loss dis zz = 1.5381\n",
      "Epoch 470 | time = 0s | loss gen = 13.9637 | loss enc = 46.6072 | loss dis = 6.8745 | loss dis xz = 0.9485 | loss dis xx = 4.4103 | loss dis zz = 1.5156\n",
      "Epoch 471 | time = 0s | loss gen = 13.4961 | loss enc = 45.6018 | loss dis = 7.4519 | loss dis xz = 0.7116 | loss dis xx = 5.2786 | loss dis zz = 1.4616\n",
      "Epoch 472 | time = 0s | loss gen = 13.2493 | loss enc = 43.5674 | loss dis = 6.8330 | loss dis xz = 0.8157 | loss dis xx = 4.5407 | loss dis zz = 1.4766\n",
      "Epoch 473 | time = 0s | loss gen = 13.2881 | loss enc = 45.1826 | loss dis = 6.7442 | loss dis xz = 0.8176 | loss dis xx = 4.4721 | loss dis zz = 1.4545\n",
      "Epoch 474 | time = 0s | loss gen = 13.9668 | loss enc = 44.6125 | loss dis = 6.6148 | loss dis xz = 0.3557 | loss dis xx = 4.7840 | loss dis zz = 1.4751\n",
      "Epoch 475 | time = 0s | loss gen = 13.3619 | loss enc = 47.0131 | loss dis = 6.6872 | loss dis xz = 0.5668 | loss dis xx = 4.6518 | loss dis zz = 1.4686\n",
      "Epoch 476 | time = 0s | loss gen = 13.5903 | loss enc = 47.3035 | loss dis = 6.5699 | loss dis xz = 0.7105 | loss dis xx = 4.4157 | loss dis zz = 1.4437\n",
      "Epoch 477 | time = 0s | loss gen = 13.1285 | loss enc = 45.3384 | loss dis = 6.8100 | loss dis xz = 0.6230 | loss dis xx = 4.7324 | loss dis zz = 1.4546\n",
      "Epoch 478 | time = 0s | loss gen = 13.5735 | loss enc = 48.5744 | loss dis = 6.3848 | loss dis xz = 0.5460 | loss dis xx = 4.3517 | loss dis zz = 1.4871\n",
      "Epoch 479 | time = 0s | loss gen = 13.1723 | loss enc = 44.1435 | loss dis = 5.7120 | loss dis xz = 0.4385 | loss dis xx = 3.8107 | loss dis zz = 1.4627\n",
      "Epoch 480 | time = 0s | loss gen = 13.8656 | loss enc = 46.0724 | loss dis = 5.5190 | loss dis xz = 0.4569 | loss dis xx = 3.6405 | loss dis zz = 1.4217\n",
      "Epoch 481 | time = 0s | loss gen = 13.4017 | loss enc = 44.4232 | loss dis = 6.6356 | loss dis xz = 0.6931 | loss dis xx = 4.4851 | loss dis zz = 1.4574\n",
      "Epoch 482 | time = 0s | loss gen = 13.3002 | loss enc = 46.1225 | loss dis = 5.7773 | loss dis xz = 0.5506 | loss dis xx = 3.7563 | loss dis zz = 1.4705\n",
      "Epoch 483 | time = 0s | loss gen = 12.3507 | loss enc = 44.7273 | loss dis = 7.0228 | loss dis xz = 0.8850 | loss dis xx = 4.6992 | loss dis zz = 1.4387\n",
      "Epoch 484 | time = 0s | loss gen = 13.8437 | loss enc = 48.8100 | loss dis = 6.3124 | loss dis xz = 0.5727 | loss dis xx = 4.3501 | loss dis zz = 1.3896\n",
      "Epoch 485 | time = 0s | loss gen = 13.8914 | loss enc = 45.8097 | loss dis = 6.3144 | loss dis xz = 0.7760 | loss dis xx = 4.0921 | loss dis zz = 1.4463\n",
      "Epoch 486 | time = 0s | loss gen = 13.5281 | loss enc = 44.5902 | loss dis = 6.5780 | loss dis xz = 0.6784 | loss dis xx = 4.4028 | loss dis zz = 1.4967\n",
      "Epoch 487 | time = 0s | loss gen = 13.9440 | loss enc = 44.7922 | loss dis = 5.9584 | loss dis xz = 0.5872 | loss dis xx = 3.9123 | loss dis zz = 1.4588\n",
      "Epoch 488 | time = 0s | loss gen = 13.4393 | loss enc = 46.2467 | loss dis = 6.6637 | loss dis xz = 0.8277 | loss dis xx = 4.3700 | loss dis zz = 1.4660\n",
      "Epoch 489 | time = 0s | loss gen = 13.9865 | loss enc = 44.2926 | loss dis = 5.6258 | loss dis xz = 0.7100 | loss dis xx = 3.4959 | loss dis zz = 1.4199\n",
      "Epoch 490 | time = 0s | loss gen = 13.1237 | loss enc = 44.8127 | loss dis = 5.8156 | loss dis xz = 0.5337 | loss dis xx = 3.8804 | loss dis zz = 1.4016\n",
      "Epoch 491 | time = 0s | loss gen = 13.2886 | loss enc = 44.6737 | loss dis = 6.4549 | loss dis xz = 0.6383 | loss dis xx = 4.4403 | loss dis zz = 1.3763\n",
      "Epoch 492 | time = 0s | loss gen = 13.8424 | loss enc = 46.1645 | loss dis = 5.8962 | loss dis xz = 0.4130 | loss dis xx = 4.0555 | loss dis zz = 1.4277\n",
      "Epoch 493 | time = 0s | loss gen = 13.1558 | loss enc = 44.0661 | loss dis = 5.9719 | loss dis xz = 0.5859 | loss dis xx = 3.9354 | loss dis zz = 1.4506\n",
      "Epoch 494 | time = 0s | loss gen = 14.2888 | loss enc = 46.5124 | loss dis = 5.7586 | loss dis xz = 0.9089 | loss dis xx = 3.3987 | loss dis zz = 1.4510\n",
      "Epoch 495 | time = 0s | loss gen = 13.9917 | loss enc = 46.4889 | loss dis = 6.1632 | loss dis xz = 0.7383 | loss dis xx = 4.0008 | loss dis zz = 1.4241\n",
      "Epoch 496 | time = 0s | loss gen = 13.5178 | loss enc = 46.1338 | loss dis = 6.5379 | loss dis xz = 0.6552 | loss dis xx = 4.4112 | loss dis zz = 1.4714\n",
      "Epoch 497 | time = 0s | loss gen = 13.8893 | loss enc = 46.2474 | loss dis = 5.9617 | loss dis xz = 0.6889 | loss dis xx = 3.8417 | loss dis zz = 1.4310\n",
      "Epoch 498 | time = 0s | loss gen = 14.0768 | loss enc = 45.7961 | loss dis = 6.0496 | loss dis xz = 0.7758 | loss dis xx = 3.8515 | loss dis zz = 1.4222\n",
      "Epoch 499 | time = 0s | loss gen = 14.3459 | loss enc = 44.0814 | loss dis = 5.1353 | loss dis xz = 0.3340 | loss dis xx = 3.4134 | loss dis zz = 1.3878\n",
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Tensor' object has no attribute 'to_proto'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'Tensor' object has no attribute 'to_proto'\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:496: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead\n",
      "WARNING:ALAD.run.arrhythmia.1:Testing evaluation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing at step 3499, method ch: Prec = 0.1471 | Rec = 0.1562 | F1 = 0.1515\n",
      "Testing at step 3499, method l1: Prec = 0.4118 | Rec = 0.4375 | F1 = 0.4242\n",
      "Testing at step 3499, method l2: Prec = 0.4412 | Rec = 0.4688 | F1 = 0.4545\n",
      "Testing at step 3499, method fm: Prec = 0.4706 | Rec = 0.5000 | F1 = 0.4848\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "from adapt_data import batch_fill\n",
    "from evaluations import save_results, heatmap\n",
    "from constants import IMAGES_DATASETS\n",
    "FREQ_PRINT = 200 # print frequency image tensorboard [20]\n",
    "FREQ_EV = 1\n",
    "PATIENCE = 10\n",
    "\n",
    "def get_getter(ema):  # to update neural net with moving avg variables, suitable for ss learning cf Saliman\n",
    "    def ema_getter(getter, name, *args, **kwargs):\n",
    "        var = getter(name, *args, **kwargs)\n",
    "        ema_var = ema.average(var)\n",
    "        return ema_var if ema_var else var\n",
    "    return ema_getter\n",
    "\n",
    "def display_parameters(batch_size, starting_lr, ema_decay, degree, label,\n",
    "                       allow_zz, score_method, do_spectral_norm):\n",
    "    \"\"\"See parameters\n",
    "    \"\"\"\n",
    "    print('Batch size: ', batch_size)\n",
    "    print('Starting learning rate: ', starting_lr)\n",
    "    print('EMA Decay: ', ema_decay)\n",
    "    print('Degree for L norms: ', degree)\n",
    "    print('Anomalous label: ', label)\n",
    "    print('Score method: ', score_method)\n",
    "    print('Discriminator zz enabled: ', allow_zz)\n",
    "    print('Spectral Norm enabled: ', do_spectral_norm)\n",
    "\n",
    "def display_progression_epoch(j, id_max):\n",
    "    \"\"\"See epoch progression\n",
    "    \"\"\"\n",
    "    batch_progression = int((j / id_max) * 100)\n",
    "    sys.stdout.write(str(batch_progression) + ' % epoch' + chr(13))\n",
    "    _ = sys.stdout.flush\n",
    "\n",
    "def create_logdir(dataset, label, rd,\n",
    "                  allow_zz, score_method, do_spectral_norm):\n",
    "    \"\"\" Directory to save training logs, weights, biases, etc.\"\"\"\n",
    "    model = 'alad_sn{}_dzz{}'.format(do_spectral_norm, allow_zz)\n",
    "    return \"train_logs/{}/{}/dzzenabled{}/{}/label{}/\" \\\n",
    "           \"rd{}\".format(dataset, model, allow_zz,\n",
    "                         score_method, label, rd)\n",
    "\n",
    "def train_and_test(dataset, nb_epochs, degree, random_seed, label,\n",
    "                   allow_zz, enable_sm, score_method,\n",
    "                   enable_early_stop, do_spectral_norm):\n",
    "    \"\"\"\n",
    "    Note:\n",
    "        Saves summaries on tensorboard. To display them, please use cmd line\n",
    "        tensorboard --logdir=model.training_logdir() --port=number\n",
    "    Args:\n",
    "        dataset (str): name of the dataset\n",
    "        nb_epochs (int): number of epochs\n",
    "        degree (int): degree of the norm in the feature matching\n",
    "        random_seed (int): trying different seeds for averaging the results\n",
    "        label (int): label which is normal for image experiments\n",
    "        allow_zz (bool): allow the d_zz discriminator or not for ablation study\n",
    "        enable_sm (bool): allow TF summaries for monitoring the training\n",
    "        score_method (str): which metric to use for the ablation study\n",
    "        enable_early_stop (bool): allow early stopping for determining the number of epochs\n",
    "        do_spectral_norm (bool): allow spectral norm or not for ablation study\n",
    "    \"\"\"\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    logger = logging.getLogger(\"ALAD.run.{}.{}\".format(\n",
    "        dataset, label))\n",
    "\n",
    "    # Import model and data\n",
    "    network = importlib.import_module('{}_utilities'.format(dataset))\n",
    "    data = arrythmia()\n",
    "    # data = importlib.import_module(\"{}\".format(dataset))\n",
    "    # Parameters\n",
    "    starting_lr = network.learning_rate\n",
    "    batch_size = network.batch_size\n",
    "    latent_dim = network.latent_dim\n",
    "    ema_decay = 0.999\n",
    "\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # Placeholders\n",
    "    x_pl = tf.placeholder(tf.float32, shape=data.get_shape_input(),\n",
    "                          name=\"input_x\")\n",
    "    z_pl = tf.placeholder(tf.float32, shape=[None, latent_dim],\n",
    "                          name=\"input_z\")\n",
    "    is_training_pl = tf.placeholder(tf.bool, [], name='is_training_pl')\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=(), name=\"lr_pl\")\n",
    "\n",
    "    # Data\n",
    "    logger.info('Data loading...')\n",
    "    trainx, trainy = data.get_train(label)\n",
    "    if enable_early_stop: validx, validy = data.get_valid(label)\n",
    "    trainx_copy = trainx.copy()\n",
    "    testx, testy = data.get_test(label)\n",
    "\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    nr_batches_train = int(trainx.shape[0] / batch_size)\n",
    "    nr_batches_test = int(testx.shape[0] / batch_size)\n",
    "\n",
    "    logger.info('Building graph...')\n",
    "\n",
    "    logger.warn(\"ALAD is training with the following parameters:\")\n",
    "    display_parameters(batch_size, starting_lr, ema_decay, degree, label,\n",
    "                       allow_zz, score_method, do_spectral_norm)\n",
    "\n",
    "    gen = network.decoder\n",
    "    enc = network.encoder\n",
    "    dis_xz = network.discriminator_xz\n",
    "    dis_xx = network.discriminator_xx\n",
    "    dis_zz = network.discriminator_zz\n",
    "\n",
    "    with tf.variable_scope('encoder_model'):\n",
    "        z_gen = enc(x_pl, is_training=is_training_pl,\n",
    "                    do_spectral_norm=do_spectral_norm)\n",
    "\n",
    "    with tf.variable_scope('generator_model'):\n",
    "        x_gen = gen(z_pl, is_training=is_training_pl)\n",
    "        rec_x = gen(z_gen, is_training=is_training_pl, reuse=True)\n",
    "\n",
    "    with tf.variable_scope('encoder_model'):\n",
    "        rec_z = enc(x_gen, is_training=is_training_pl, reuse=True,\n",
    "                    do_spectral_norm=do_spectral_norm)\n",
    "\n",
    "    with tf.variable_scope('discriminator_model_xz'):\n",
    "        l_encoder, inter_layer_inp_xz = dis_xz(x_pl, z_gen,\n",
    "                                            is_training=is_training_pl,\n",
    "                    do_spectral_norm=do_spectral_norm)\n",
    "        l_generator, inter_layer_rct_xz = dis_xz(x_gen, z_pl,\n",
    "                                              is_training=is_training_pl,\n",
    "                                              reuse=True,\n",
    "                    do_spectral_norm=do_spectral_norm)\n",
    "\n",
    "    with tf.variable_scope('discriminator_model_xx'):\n",
    "        x_logit_real, inter_layer_inp_xx = dis_xx(x_pl, x_pl,\n",
    "                                                  is_training=is_training_pl,\n",
    "                    do_spectral_norm=do_spectral_norm)\n",
    "        x_logit_fake, inter_layer_rct_xx = dis_xx(x_pl, rec_x, is_training=is_training_pl,\n",
    "                              reuse=True, do_spectral_norm=do_spectral_norm)\n",
    "\n",
    "    with tf.variable_scope('discriminator_model_zz'):\n",
    "        z_logit_real, _ = dis_zz(z_pl, z_pl, is_training=is_training_pl,\n",
    "                                 do_spectral_norm=do_spectral_norm)\n",
    "        z_logit_fake, _ = dis_zz(z_pl, rec_z, is_training=is_training_pl,\n",
    "                              reuse=True, do_spectral_norm=do_spectral_norm)\n",
    "\n",
    "    with tf.name_scope('loss_functions'):\n",
    "\n",
    "        # discriminator xz\n",
    "        loss_dis_enc = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.ones_like(l_encoder),logits=l_encoder))\n",
    "        loss_dis_gen = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.zeros_like(l_generator),logits=l_generator))\n",
    "        dis_loss_xz = loss_dis_gen + loss_dis_enc\n",
    "\n",
    "        # discriminator xx\n",
    "        x_real_dis = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=x_logit_real, labels=tf.ones_like(x_logit_real))\n",
    "        x_fake_dis = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=x_logit_fake, labels=tf.zeros_like(x_logit_fake))\n",
    "        dis_loss_xx = tf.reduce_mean(x_real_dis + x_fake_dis)\n",
    "\n",
    "        # discriminator zz\n",
    "        z_real_dis = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=z_logit_real, labels=tf.ones_like(z_logit_real))\n",
    "        z_fake_dis = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=z_logit_fake, labels=tf.zeros_like(z_logit_fake))\n",
    "        dis_loss_zz = tf.reduce_mean(z_real_dis + z_fake_dis)\n",
    "\n",
    "        loss_discriminator = dis_loss_xz + dis_loss_xx + dis_loss_zz if \\\n",
    "            allow_zz else dis_loss_xz + dis_loss_xx\n",
    "\n",
    "        # generator and encoder\n",
    "        gen_loss_xz = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.ones_like(l_generator),logits=l_generator))\n",
    "        enc_loss_xz = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.zeros_like(l_encoder), logits=l_encoder))\n",
    "        x_real_gen = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=x_logit_real, labels=tf.zeros_like(x_logit_real))\n",
    "        x_fake_gen = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=x_logit_fake, labels=tf.ones_like(x_logit_fake))\n",
    "        z_real_gen = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=z_logit_real, labels=tf.zeros_like(z_logit_real))\n",
    "        z_fake_gen = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=z_logit_fake, labels=tf.ones_like(z_logit_fake))\n",
    "\n",
    "        cost_x = tf.reduce_mean(x_real_gen + x_fake_gen)\n",
    "        cost_z = tf.reduce_mean(z_real_gen + z_fake_gen)\n",
    "\n",
    "        cycle_consistency_loss = cost_x + cost_z if allow_zz else cost_x\n",
    "        loss_generator = gen_loss_xz + cycle_consistency_loss\n",
    "        loss_encoder = enc_loss_xz + cycle_consistency_loss\n",
    "\n",
    "    with tf.name_scope('optimizers'):\n",
    "\n",
    "        # control op dependencies for batch norm and trainable variables\n",
    "        tvars = tf.trainable_variables()\n",
    "        dxzvars = [var for var in tvars if 'discriminator_model_xz' in var.name]\n",
    "        dxxvars = [var for var in tvars if 'discriminator_model_xx' in var.name]\n",
    "        dzzvars = [var for var in tvars if 'discriminator_model_zz' in var.name]\n",
    "        gvars = [var for var in tvars if 'generator_model' in var.name]\n",
    "        evars = [var for var in tvars if 'encoder_model' in var.name]\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        update_ops_gen = [x for x in update_ops if ('generator_model' in x.name)]\n",
    "        update_ops_enc = [x for x in update_ops if ('encoder_model' in x.name)]\n",
    "        update_ops_dis_xz = [x for x in update_ops if\n",
    "                             ('discriminator_model_xz' in x.name)]\n",
    "        update_ops_dis_xx = [x for x in update_ops if\n",
    "                             ('discriminator_model_xx' in x.name)]\n",
    "        update_ops_dis_zz = [x for x in update_ops if\n",
    "                             ('discriminator_model_zz' in x.name)]\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
    "                                                  beta1=0.5)\n",
    "\n",
    "        with tf.control_dependencies(update_ops_gen):\n",
    "            gen_op = optimizer.minimize(loss_generator, var_list=gvars,\n",
    "                                            global_step=global_step)\n",
    "        with tf.control_dependencies(update_ops_enc):\n",
    "            enc_op = optimizer.minimize(loss_encoder, var_list=evars)\n",
    "\n",
    "        with tf.control_dependencies(update_ops_dis_xz):\n",
    "            dis_op_xz = optimizer.minimize(dis_loss_xz, var_list=dxzvars)\n",
    "\n",
    "        with tf.control_dependencies(update_ops_dis_xx):\n",
    "            dis_op_xx = optimizer.minimize(dis_loss_xx, var_list=dxxvars)\n",
    "\n",
    "        with tf.control_dependencies(update_ops_dis_zz):\n",
    "            dis_op_zz = optimizer.minimize(dis_loss_zz, var_list=dzzvars)\n",
    "\n",
    "        # Exponential Moving Average for inference\n",
    "        def train_op_with_ema_dependency(vars, op):\n",
    "            ema = tf.train.ExponentialMovingAverage(decay=ema_decay)\n",
    "            maintain_averages_op = ema.apply(vars)\n",
    "            with tf.control_dependencies([op]):\n",
    "                train_op = tf.group(maintain_averages_op)\n",
    "            return train_op, ema\n",
    "\n",
    "        train_gen_op, gen_ema = train_op_with_ema_dependency(gvars, gen_op)\n",
    "        train_enc_op, enc_ema = train_op_with_ema_dependency(evars, enc_op)\n",
    "        train_dis_op_xz, xz_ema = train_op_with_ema_dependency(dxzvars,\n",
    "                                                               dis_op_xz)\n",
    "        train_dis_op_xx, xx_ema = train_op_with_ema_dependency(dxxvars,\n",
    "                                                               dis_op_xx)\n",
    "        train_dis_op_zz, zz_ema = train_op_with_ema_dependency(dzzvars,\n",
    "                                                               dis_op_zz)\n",
    "\n",
    "    with tf.variable_scope('encoder_model'):\n",
    "        z_gen_ema = enc(x_pl, is_training=is_training_pl,\n",
    "                        getter=get_getter(enc_ema), reuse=True,\n",
    "                        do_spectral_norm=do_spectral_norm)\n",
    "\n",
    "    with tf.variable_scope('generator_model'):\n",
    "        rec_x_ema = gen(z_gen_ema, is_training=is_training_pl,\n",
    "                              getter=get_getter(gen_ema), reuse=True)\n",
    "        x_gen_ema = gen(z_pl, is_training=is_training_pl,\n",
    "                              getter=get_getter(gen_ema), reuse=True)\n",
    "\n",
    "    with tf.variable_scope('discriminator_model_xx'):\n",
    "        l_encoder_emaxx, inter_layer_inp_emaxx = dis_xx(x_pl, x_pl,\n",
    "                                                    is_training=is_training_pl,\n",
    "                                                    getter=get_getter(xx_ema),\n",
    "                                                    reuse=True,\n",
    "                    do_spectral_norm=do_spectral_norm)\n",
    "\n",
    "        l_generator_emaxx, inter_layer_rct_emaxx = dis_xx(x_pl, rec_x_ema,\n",
    "                                                      is_training=is_training_pl,\n",
    "                                                      getter=get_getter(\n",
    "                                                          xx_ema),\n",
    "                                                      reuse=True,\n",
    "                    do_spectral_norm=do_spectral_norm)\n",
    "\n",
    "    with tf.name_scope('Testing'):\n",
    "\n",
    "        with tf.variable_scope('Scores'):\n",
    "\n",
    "\n",
    "            score_ch = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=tf.ones_like(l_generator_emaxx),\n",
    "                    logits=l_generator_emaxx)\n",
    "            score_ch = tf.squeeze(score_ch)\n",
    "\n",
    "            rec = x_pl - rec_x_ema\n",
    "            rec = tf.contrib.layers.flatten(rec)\n",
    "            score_l1 = tf.norm(rec, ord=1, axis=1,\n",
    "                            keep_dims=False, name='d_loss')\n",
    "            score_l1 = tf.squeeze(score_l1)\n",
    "\n",
    "            rec = x_pl - rec_x_ema\n",
    "            rec = tf.contrib.layers.flatten(rec)\n",
    "            score_l2 = tf.norm(rec, ord=2, axis=1,\n",
    "                            keep_dims=False, name='d_loss')\n",
    "            score_l2 = tf.squeeze(score_l2)\n",
    "\n",
    "            inter_layer_inp, inter_layer_rct = inter_layer_inp_emaxx, \\\n",
    "                                               inter_layer_rct_emaxx\n",
    "            fm = inter_layer_inp - inter_layer_rct\n",
    "            fm = tf.contrib.layers.flatten(fm)\n",
    "            score_fm = tf.norm(fm, ord=degree, axis=1,\n",
    "                             keep_dims=False, name='d_loss')\n",
    "            score_fm = tf.squeeze(score_fm)\n",
    "\n",
    "    if enable_early_stop:\n",
    "        rec_error_valid = tf.reduce_mean(score_fm)\n",
    "\n",
    "    if enable_sm:\n",
    "\n",
    "        with tf.name_scope('summary'):\n",
    "            with tf.name_scope('dis_summary'):\n",
    "                tf.summary.scalar('loss_discriminator', loss_discriminator, ['dis'])\n",
    "                tf.summary.scalar('loss_dis_encoder', loss_dis_enc, ['dis'])\n",
    "                tf.summary.scalar('loss_dis_gen', loss_dis_gen, ['dis'])\n",
    "                tf.summary.scalar('loss_dis_xz', dis_loss_xz, ['dis'])\n",
    "                tf.summary.scalar('loss_dis_xx', dis_loss_xx, ['dis'])\n",
    "                if allow_zz:\n",
    "                    tf.summary.scalar('loss_dis_zz', dis_loss_zz, ['dis'])\n",
    "\n",
    "            with tf.name_scope('gen_summary'):\n",
    "                tf.summary.scalar('loss_generator', loss_generator, ['gen'])\n",
    "                tf.summary.scalar('loss_encoder', loss_encoder, ['gen'])\n",
    "                tf.summary.scalar('loss_encgen_dxx', cost_x, ['gen'])\n",
    "                if allow_zz:\n",
    "                    tf.summary.scalar('loss_encgen_dzz', cost_z, ['gen'])\n",
    "\n",
    "            if enable_early_stop:\n",
    "                with tf.name_scope('validation_summary'):\n",
    "                   tf.summary.scalar('valid', rec_error_valid, ['v'])\n",
    "\n",
    "            with tf.name_scope('img_summary'):\n",
    "                heatmap_pl_latent = tf.placeholder(tf.float32,\n",
    "                                                   shape=(1, 480, 640, 3),\n",
    "                                                   name=\"heatmap_pl_latent\")\n",
    "                sum_op_latent = tf.summary.image('heatmap_latent', heatmap_pl_latent)\n",
    "\n",
    "            if dataset in IMAGES_DATASETS:\n",
    "                with tf.name_scope('image_summary'):\n",
    "                    tf.summary.image('reconstruct', rec_x, 8, ['image'])\n",
    "                    tf.summary.image('input_images', x_pl, 8, ['image'])\n",
    "\n",
    "            else:\n",
    "                heatmap_pl_rec = tf.placeholder(tf.float32, shape=(1, 480, 640, 3),\n",
    "                                            name=\"heatmap_pl_rec\")\n",
    "                with tf.name_scope('image_summary'):\n",
    "                    tf.summary.image('heatmap_rec', heatmap_pl_rec, 1, ['image'])\n",
    "\n",
    "            sum_op_dis = tf.summary.merge_all('dis')\n",
    "            sum_op_gen = tf.summary.merge_all('gen')\n",
    "            sum_op = tf.summary.merge([sum_op_dis, sum_op_gen])\n",
    "            sum_op_im = tf.summary.merge_all('image')\n",
    "            sum_op_valid = tf.summary.merge_all('v')\n",
    "\n",
    "    logdir = create_logdir(dataset, label, random_seed, allow_zz, score_method,\n",
    "                           do_spectral_norm)\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "    save_model_secs = None if enable_early_stop else 20\n",
    "    sv = tf.train.Supervisor(logdir=logdir, save_summaries_secs=None, saver=saver, save_model_secs=save_model_secs) \n",
    "\n",
    "    logger.info('Start training...')\n",
    "    with sv.managed_session(config=config) as sess:\n",
    "\n",
    "        step = sess.run(global_step)\n",
    "        logger.info('Initialization done at step {}'.format(step/nr_batches_train))\n",
    "        writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "        train_batch = 0\n",
    "        epoch = 0\n",
    "        best_valid_loss = 0\n",
    "        request_stop = False\n",
    "\n",
    "        while not sv.should_stop() and epoch < nb_epochs:\n",
    "\n",
    "            lr = starting_lr\n",
    "            begin = time.time()\n",
    "\n",
    "             # construct randomly permuted minibatches\n",
    "            trainx = trainx[rng.permutation(trainx.shape[0])]  # shuffling dataset\n",
    "            trainx_copy = trainx_copy[rng.permutation(trainx.shape[0])]\n",
    "            train_loss_dis_xz, train_loss_dis_xx,  train_loss_dis_zz, \\\n",
    "            train_loss_dis, train_loss_gen, train_loss_enc = [0, 0, 0, 0, 0, 0]\n",
    "\n",
    "            # Training\n",
    "            for t in range(nr_batches_train):\n",
    "\n",
    "                display_progression_epoch(t, nr_batches_train)\n",
    "                ran_from = t * batch_size\n",
    "                ran_to = (t + 1) * batch_size\n",
    "\n",
    "                # train discriminator\n",
    "                feed_dict = {x_pl: trainx[ran_from:ran_to],\n",
    "                             z_pl: np.random.normal(size=[batch_size, latent_dim]),\n",
    "                             is_training_pl: True,\n",
    "                             learning_rate:lr}\n",
    "\n",
    "                _, _, _, ld, ldxz, ldxx, ldzz, step = sess.run([train_dis_op_xz,\n",
    "                                                              train_dis_op_xx,\n",
    "                                                              train_dis_op_zz,\n",
    "                                                              loss_discriminator,\n",
    "                                                              dis_loss_xz,\n",
    "                                                              dis_loss_xx,\n",
    "                                                              dis_loss_zz,\n",
    "                                                              global_step],\n",
    "                                                             feed_dict=feed_dict)\n",
    "                train_loss_dis += ld\n",
    "                train_loss_dis_xz += ldxz\n",
    "                train_loss_dis_xx += ldxx\n",
    "                train_loss_dis_zz += ldzz\n",
    "\n",
    "                # train generator and encoder\n",
    "                feed_dict = {x_pl: trainx_copy[ran_from:ran_to],\n",
    "                             z_pl: np.random.normal(size=[batch_size, latent_dim]),\n",
    "                             is_training_pl: True,\n",
    "                             learning_rate:lr}\n",
    "                _,_, le, lg = sess.run([train_gen_op,\n",
    "                                            train_enc_op,\n",
    "                                            loss_encoder,\n",
    "                                            loss_generator],\n",
    "                                           feed_dict=feed_dict)\n",
    "                train_loss_gen += lg\n",
    "                train_loss_enc += le\n",
    "\n",
    "                if enable_sm:\n",
    "                    sm = sess.run(sum_op, feed_dict=feed_dict)\n",
    "                    writer.add_summary(sm, step)\n",
    "\n",
    "                    if t % FREQ_PRINT == 0 and dataset in IMAGES_DATASETS:  # inspect reconstruction\n",
    "                        t = np.random.randint(0, trainx.shape[0]-batch_size)\n",
    "                        ran_from = t\n",
    "                        ran_to = t + batch_size\n",
    "                        feed_dict = {x_pl: trainx[ran_from:ran_to],\n",
    "                            z_pl: np.random.normal(\n",
    "                                size=[batch_size, latent_dim]),\n",
    "                            is_training_pl: False}\n",
    "                        sm = sess.run(sum_op_im, feed_dict=feed_dict)\n",
    "                        writer.add_summary(sm, step)#train_batch)\n",
    "\n",
    "                train_batch += 1\n",
    "\n",
    "            train_loss_gen /= nr_batches_train\n",
    "            train_loss_enc /= nr_batches_train\n",
    "            train_loss_dis /= nr_batches_train\n",
    "            train_loss_dis_xz /= nr_batches_train\n",
    "            train_loss_dis_xx /= nr_batches_train\n",
    "            train_loss_dis_zz /= nr_batches_train\n",
    "\n",
    "            logger.info('Epoch terminated')\n",
    "            if allow_zz:\n",
    "                print(\"Epoch %d | time = %ds | loss gen = %.4f | loss enc = %.4f | \"\n",
    "                      \"loss dis = %.4f | loss dis xz = %.4f | loss dis xx = %.4f | \"\n",
    "                      \"loss dis zz = %.4f\"\n",
    "                      % (epoch, time.time() - begin, train_loss_gen,\n",
    "                         train_loss_enc, train_loss_dis, train_loss_dis_xz,\n",
    "                         train_loss_dis_xx, train_loss_dis_zz))\n",
    "            else:\n",
    "                print(\"Epoch %d | time = %ds | loss gen = %.4f | loss enc = %.4f | \"\n",
    "                      \"loss dis = %.4f | loss dis xz = %.4f | loss dis xx = %.4f | \"\n",
    "                      % (epoch, time.time() - begin, train_loss_gen,\n",
    "                         train_loss_enc, train_loss_dis, train_loss_dis_xz,\n",
    "                         train_loss_dis_xx))\n",
    "\n",
    "            ##EARLY STOPPING\n",
    "            if (epoch + 1) % FREQ_EV == 0 and enable_early_stop:\n",
    "\n",
    "                valid_loss = 0\n",
    "                feed_dict = {x_pl: validx,\n",
    "                             z_pl: np.random.normal(size=[validx.shape[0], latent_dim]),\n",
    "                             is_training_pl: False}\n",
    "                vl, lat = sess.run([rec_error_valid, rec_z], feed_dict=feed_dict)\n",
    "                valid_loss += vl\n",
    "\n",
    "                if enable_sm:\n",
    "                    sm = sess.run(sum_op_valid, feed_dict=feed_dict)\n",
    "                    writer.add_summary(sm, step)  # train_batch)\n",
    "\n",
    "                logger.info('Validation: valid loss {:.4f}'.format(valid_loss))\n",
    "\n",
    "                if (valid_loss < best_valid_loss or epoch == FREQ_EV-1):\n",
    "                    best_valid_loss = valid_loss\n",
    "                    logger.info(\"Best model - valid loss = {:.4f} - saving...\".format(best_valid_loss))\n",
    "                    sv.saver.save(sess, logdir+'/model.ckpt', global_step=step)\n",
    "                    nb_without_improvements = 0\n",
    "                else:\n",
    "                    nb_without_improvements += FREQ_EV\n",
    "\n",
    "                if nb_without_improvements > PATIENCE:\n",
    "                    sv.request_stop()\n",
    "                    logger.warning(\n",
    "                      \"Early stopping at epoch {} with weights from epoch {}\".format(\n",
    "                          epoch, epoch - nb_without_improvements))\n",
    "\n",
    "            epoch += 1\n",
    "\n",
    "        sv.saver.save(sess, logdir+'/model.ckpt', global_step=step)\n",
    "\n",
    "        logger.warn('Testing evaluation...')\n",
    "\n",
    "        scores_ch = []\n",
    "        scores_l1 = []\n",
    "        scores_l2 = []\n",
    "        scores_fm = []\n",
    "        inference_time = []\n",
    "\n",
    "        # Create scores\n",
    "        for t in range(nr_batches_test):\n",
    "\n",
    "            # construct randomly permuted minibatches\n",
    "            ran_from = t * batch_size\n",
    "            ran_to = (t + 1) * batch_size\n",
    "            begin_test_time_batch = time.time()\n",
    "\n",
    "            feed_dict = {x_pl: testx[ran_from:ran_to],\n",
    "                         z_pl: np.random.normal(size=[batch_size, latent_dim]),\n",
    "                         is_training_pl:False}\n",
    "\n",
    "            scores_ch += sess.run(score_ch, feed_dict=feed_dict).tolist()\n",
    "            scores_l1 += sess.run(score_l1, feed_dict=feed_dict).tolist()\n",
    "            scores_l2 += sess.run(score_l2, feed_dict=feed_dict).tolist()\n",
    "            scores_fm += sess.run(score_fm, feed_dict=feed_dict).tolist()\n",
    "            inference_time.append(time.time() - begin_test_time_batch)\n",
    "\n",
    "\n",
    "        inference_time = np.mean(inference_time)\n",
    "        logger.info('Testing : mean inference time is %.4f' % (inference_time))\n",
    "\n",
    "        if testx.shape[0] % batch_size != 0:\n",
    "\n",
    "            batch, size = batch_fill(testx, batch_size)\n",
    "            feed_dict = {x_pl: batch,\n",
    "                         z_pl: np.random.normal(size=[batch_size, latent_dim]),\n",
    "                         is_training_pl: False}\n",
    "\n",
    "            bscores_ch = sess.run(score_ch,feed_dict=feed_dict).tolist()\n",
    "            bscores_l1 = sess.run(score_l1,feed_dict=feed_dict).tolist()\n",
    "            bscores_l2 = sess.run(score_l2,feed_dict=feed_dict).tolist()\n",
    "            bscores_fm = sess.run(score_fm,feed_dict=feed_dict).tolist()\n",
    "\n",
    "\n",
    "            scores_ch += bscores_ch[:size]\n",
    "            scores_l1 += bscores_l1[:size]\n",
    "            scores_l2 += bscores_l2[:size]\n",
    "            scores_fm += bscores_fm[:size]\n",
    "\n",
    "        model = 'alad_sn{}_dzz{}'.format(do_spectral_norm, allow_zz)\n",
    "        save_results(scores_ch, testy, model, dataset, 'ch',\n",
    "                     'dzzenabled{}'.format(allow_zz), label, random_seed, step)\n",
    "        save_results(scores_l1, testy, model, dataset, 'l1',\n",
    "                     'dzzenabled{}'.format(allow_zz), label, random_seed, step)\n",
    "        save_results(scores_l2, testy, model, dataset, 'l2',\n",
    "                     'dzzenabled{}'.format(allow_zz), label, random_seed, step)\n",
    "        save_results(scores_fm, testy, model, dataset, 'fm',\n",
    "                     'dzzenabled{}'.format(allow_zz), label, random_seed,  step)\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    train_and_test(dataset=\"arrhythmia\",nb_epochs=500,degree=2,random_seed=2\n",
    "            ,label=1,allow_zz=True,enable_sm=True,score_method=\"\"\n",
    "            ,enable_early_stop=False,do_spectral_norm=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ALAD.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "mount_file_id": "https://github.com/zahraDehghanian97/Adversarially-Learned-Anomaly-Detection/blob/master/alad/ALAD.ipynb",
   "authorship_tag": "ABX9TyMfUalp7o26EeEjHW1a85pc",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
