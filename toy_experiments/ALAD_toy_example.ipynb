{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "ALAD - toy example.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahraDehghanian97/Adversarially-Learned-Anomaly-Detection/blob/master/toy_experiments/ALAD_toy_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPnFgZ6Z2EYC"
      },
      "source": [
        "Add git repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIUvh65REtC9",
        "outputId": "58701ce8-3c66-4940-b5f8-9f58724f986c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru5hpU51rJdI",
        "outputId": "cc8f4b69-9cad-47ef-9532-4191291f502a"
      },
      "source": [
        "# Clone github repository setup\n",
        "from os.path import join  \n",
        "\n",
        "# path to your project on Google Drive\n",
        "MY_GOOGLE_DRIVE_PATH = 'drive/MyDrive/Colab Notebooks/Adversarially-Learned-Anomaly-Detection' \n",
        "# replace with your Github username \n",
        "GIT_USERNAME = \"zahraDehghanian97\" \n",
        "# definitely replace with your\n",
        "GIT_TOKEN = \"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\"\n",
        "# Replace with your github repository \n",
        "GIT_REPOSITORY = \"Adversarially-Learned-Anomaly-Detection\" \n",
        "\n",
        "PROJECT_PATH = MY_GOOGLE_DRIVE_PATH\n",
        "\n",
        "# It's good to print out the value if you are not sure \n",
        "print(\"PROJECT_PATH: \", PROJECT_PATH)   \n",
        "\n",
        "# # In case we haven't created the folder already; we will create a folder in the project path \n",
        "# !mkdir \"{PROJECT_PATH}\"    \n",
        "\n",
        "#GIT_PATH = \"https://{GIT_TOKEN}@github.com/{GIT_USERNAME}/{GIT_REPOSITORY}.git\" this return 400 Bad Request for me\n",
        "GIT_PATH = \"https://\" + GIT_TOKEN + \"@github.com/\" + GIT_USERNAME + \"/\" + GIT_REPOSITORY + \".git\"\n",
        "print(\"GIT_PATH: \", GIT_PATH)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROJECT_PATH:  drive/MyDrive/Colab Notebooks/Adversarially-Learned-Anomaly-Detection\n",
            "GIT_PATH:  https://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@github.com/zahraDehghanian97/Adversarially-Learned-Anomaly-Detection.git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOwduLdy2kjX"
      },
      "source": [
        "run just one time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfMhwAyssYBH",
        "outputId": "83cbbd93-4356-44c1-d2a9-f115ace350a7"
      },
      "source": [
        "%ls\n",
        "%cd \"{PROJECT_PATH}\" \n",
        "%cd \"../\"\n",
        "!git clone \"{GIT_PATH}\" # clone the github repository"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n",
            "/content/drive/MyDrive/Colab Notebooks/Adversarially-Learned-Anomaly-Detection\n",
            "/content/drive/MyDrive/Colab Notebooks\n",
            "fatal: destination path 'Adversarially-Learned-Anomaly-Detection' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSLK5TkqwRog",
        "outputId": "498a29c4-2745-471b-9efb-4d9110a42096"
      },
      "source": [
        "!git status"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch master\n",
            "Your branch is up to date with 'origin/master'.\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add/rm <file>...\" to update what will be committed)\n",
            "  (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
            "\n",
            "\t\u001b[31mdeleted:    ALAD_toy_example.ipynb\u001b[m\n",
            "\t\u001b[31mmodified:   toy_experiments/ALAD - toy example.ipynb\u001b[m\n",
            "\t\u001b[31mdeleted:    toy_experiments/ALAD_toy_example.ipynb\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w9MabeeyfBo"
      },
      "source": [
        "!git config --global user.email \"zahra.dehghanian97@gmail.com\"\n",
        "!git config --global user.name \"zahraDehghanian97\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_-Zgghg2okW"
      },
      "source": [
        "run each time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnwnAQRMs7vc",
        "outputId": "5b0d5803-f40b-4be6-ae7e-82dc4ad4cd4b"
      },
      "source": [
        "%ls\n",
        "%cd \"{PROJECT_PATH}\" \n",
        "%ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34malad\u001b[0m/    \u001b[01;34mdagmm\u001b[0m/  \u001b[01;34mdsebm\u001b[0m/   main.py    requirements.txt  \u001b[01;34mutils\u001b[0m/\n",
            "\u001b[01;34manogan\u001b[0m/  \u001b[01;34mdata\u001b[0m/   LICENSE  README.md  \u001b[01;34mtoy_experiments\u001b[0m/\n",
            "[Errno 2] No such file or directory: 'drive/MyDrive/Colab Notebooks/Adversarially-Learned-Anomaly-Detection'\n",
            "/content/drive/My Drive/Colab Notebooks/Adversarially-Learned-Anomaly-Detection\n",
            "\u001b[0m\u001b[01;34malad\u001b[0m/    \u001b[01;34mdagmm\u001b[0m/  \u001b[01;34mdsebm\u001b[0m/   main.py    requirements.txt  \u001b[01;34mutils\u001b[0m/\n",
            "\u001b[01;34manogan\u001b[0m/  \u001b[01;34mdata\u001b[0m/   LICENSE  README.md  \u001b[01;34mtoy_experiments\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88cGfgCGxtlh"
      },
      "source": [
        "!git add ."
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuCuO7R6yVQ0",
        "outputId": "bfe2c58a-0fb7-4ec4-c9dc-0ba84691e348"
      },
      "source": [
        "!git commit -m \"test\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[master 4e3a383] test\n",
            " 3 files changed, 1 insertion(+), 9244 deletions(-)\n",
            " delete mode 100644 ALAD_toy_example.ipynb\n",
            " rewrite toy_experiments/ALAD - toy example.ipynb (99%)\n",
            " delete mode 100644 toy_experiments/ALAD_toy_example.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIPsy5r20_5l",
        "outputId": "87e7989c-dd04-4b7e-8e8d-46b7e906fd5f"
      },
      "source": [
        "!git push -u origin master"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: could not read Password for 'https://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@github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUD-sPeYPJpr"
      },
      "source": [
        "# ALAD - Toy examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhPVlkptPJpw"
      },
      "source": [
        "This is the notebook that generated Figure 2 in \"Adversarially learnt anomaly detection\"\n",
        "Credits : https://github.com/ChunyuanLI/ALICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksgJuh59PJpw"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "i-SbCfCHPJpx"
      },
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import pdb\n",
        "import sys\n",
        "py_file_location = \"/content/drive/MyDrive/Colab Notebooks/Adversarially-Learned-Anomaly-Detection-master/toy_experiments/utils\"\n",
        "sys.path.append(os.path.abspath(py_file_location))\n",
        "print(sys.path)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2wnZs5zi7-ZV",
        "outputId": "1e70b188-2498-4100-8143-a596793b2469"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "uqfGnfufPJpy",
        "outputId": "768c4298-ae11-4bda-848c-96ea1e42efdd"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import tensorflow as tf\n",
        "from data_gmm import GMM_distribution, sample_GMM, plot_GMM\n",
        "from data_utils import shuffle, iter_data, ToyDataset\n",
        "from tqdm import tqdm\n",
        "import sklearn.datasets\n",
        "\n",
        "slim = tf.contrib.slim\n",
        "ds = tf.contrib.distributions\n",
        "graph_replace = tf.contrib.graph_editor.graph_replace"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-859140d90169>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_gmm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGMM_distribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_GMM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_GMM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToyDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_gmm'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOkFCsxsPJpz"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "oowqseLQPJpz"
      },
      "source": [
        "DATASET = '4gaussians' # 5gaussians, swiss_roll, s_curve\n",
        "N_NOISY = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "R_2dRKgyPJp0"
      },
      "source": [
        "\"\"\" parameters \"\"\"\n",
        "n_epoch = 300\n",
        "batch_size  = 1024\n",
        "dataset_size_x = 512*4\n",
        "dataset_size_z = 512*4\n",
        "\n",
        "dataset_size_x_test = 512*2\n",
        "dataset_size_z_test = 512*2\n",
        "\n",
        "input_dim = 2\n",
        "latent_dim = 2\n",
        "eps_dim = 2\n",
        "\n",
        "n_layer_disc  = 2\n",
        "n_hidden_disc = 256\n",
        "n_layer_gen   = 3\n",
        "n_hidden_gen  = 256\n",
        "n_layer_inf   = 2\n",
        "n_hidden_inf  = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "MXdd3mJXPJp2"
      },
      "source": [
        "\"\"\" Create directory for results \"\"\"\n",
        "result_dir = 'results/alad_toy/'\n",
        "directory = result_dir\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUBITKBDPJp4"
      },
      "source": [
        "## Training dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfEiP1QAPJp4"
      },
      "source": [
        "#### 4 or 5 GMM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "vOgUSPBuPJp5"
      },
      "source": [
        "\"\"\" Create dataset \"\"\"\n",
        "\n",
        "def four_five_gaussians(p1=0):\n",
        "    # create X dataset\n",
        "    global dataset_size_x\n",
        "    means_x = map(lambda x:  np.array(x), [[2, 2],[-2, -2],[2, -2],[-2, 2],[0, 0]])\n",
        "    means_x = list(means_x)\n",
        "    std_x = 0.02\n",
        "    variances_x = [np.eye(2) * std_x for _ in means_x]\n",
        "    #contamination = 4.0*p/(1-p)\n",
        "    priors_x = np.array([1.0, 1.0, 1.0, 1.0, p1])\n",
        "    priors_x /= sum(priors_x)\n",
        "    #print(priors_x)\n",
        "    gaussian_mixture = GMM_distribution(means=means_x,variances=variances_x,priors=priors_x)\n",
        "    dataset_x = sample_GMM(dataset_size_x, means_x, variances_x, priors_x, sources=('features', ))\n",
        "    return dataset_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34_xFzKlPJp5"
      },
      "source": [
        "You can control the level of the fifth gaussian in the training set with the $p$ parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsCaQ21UPJp6"
      },
      "source": [
        "Generate training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "uLhJicsSPJp6"
      },
      "source": [
        "dataset_x = four_five_gaussians(p1=0)\n",
        "save_path_x = result_dir + 'X_4gmm_data_train.png'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxuwMzyCPJp6"
      },
      "source": [
        "## input x\n",
        "X_dataset  = dataset_x.data['samples']\n",
        "X_targets = dataset_x.data['label']\n",
        "\n",
        "fig_mx, ax = plt.subplots(nrows=1, ncols=1, figsize=(4.5, 4.5))\n",
        "ax.scatter(X_dataset[:, 0], X_dataset[:, 1], c=cm.tab20(X_targets.astype(float)/input_dim/3.0),\n",
        "           edgecolor='none', alpha=0.5)\n",
        "#ax.set_xlim(-3, 3); ax.set_ylim(-3.5, 3.5)\n",
        "ax.set_xlabel('$x_1$'); ax.set_ylabel('$x_2$')\n",
        "ax.set_title(\"X distribution in the training set\" )\n",
        "ax.axis('on')\n",
        "plt.savefig(save_path_x, transparent=True, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzduWLEgPJp7"
      },
      "source": [
        "# create Z dataset\n",
        "means_z = map(lambda x:  np.array(x), [[0, 0]])\n",
        "means_z = list(means_z)\n",
        "std_z = 1.0\n",
        "variances_z = [np.eye(2) * std_z for _ in means_z]\n",
        "priors_z = [1.0/len(means_z) for _ in means_z]\n",
        "\n",
        "dataset_z = sample_GMM(dataset_size_z, means_z, variances_z, priors_z, sources=('features', ))\n",
        "save_path_z = result_dir + 'Z_gmm_data_train.png'\n",
        "\n",
        "##  input z\n",
        "Z_dataset = dataset_z.data['samples']\n",
        "Z_labels  = dataset_z.data['label']\n",
        "\n",
        "fig_mx, ax = plt.subplots(nrows=1, ncols=1, figsize=(4.5, 4.5))\n",
        "ax.scatter(Z_dataset[:, 0], Z_dataset[:, 1],\n",
        "           edgecolor='none', alpha=0.5)\n",
        "ax.set_xlim(-3, 3); ax.set_ylim(-3.5, 3.5)\n",
        "ax.set_xlabel('$z_1$'); ax.set_ylabel('$z_2$')\n",
        "ax.set_title(\"Z distribution in the training set\")\n",
        "ax.axis('on')\n",
        "plt.savefig(save_path_z, transparent=True, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STBWBPb1PJp8"
      },
      "source": [
        "## Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "XejE1et6PJp8"
      },
      "source": [
        "\"\"\" Networks \"\"\"\n",
        "\n",
        "def generative_network(z, input_dim, n_layer, n_hidden, eps_dim, reuse=False):\n",
        "    with tf.variable_scope(\"generative\", reuse=reuse):\n",
        "        h = slim.stack(z, slim.fully_connected, [n_hidden] * n_layer, activation_fn=tf.nn.relu)\n",
        "        x = slim.fully_connected(h, input_dim, activation_fn=None)\n",
        "    return x\n",
        "\n",
        "\n",
        "def inference_network(x, latent_dim, n_layer, n_hidden, eps_dim, reuse=False):\n",
        "    with tf.variable_scope(\"inference\", reuse=reuse):\n",
        "        h = slim.stack(x, slim.fully_connected, [n_hidden] * n_layer, activation_fn=tf.nn.relu)\n",
        "        z = slim.fully_connected(h, latent_dim, activation_fn=None)      \n",
        "    return z\n",
        "\n",
        "def data_network_xz(x, z, n_layers=2, n_hidden=128, activation_fn=None,reuse=False):\n",
        "    \"\"\"Approximate x log data density.\"\"\"\n",
        "    h = tf.concat([x,z], 1)\n",
        "    with tf.variable_scope('discriminator', reuse=reuse):\n",
        "        h = slim.stack(h, slim.fully_connected, [n_hidden] * n_layers, activation_fn=tf.nn.relu)\n",
        "        log_d = slim.fully_connected(h, 1, activation_fn=None)\n",
        "    return tf.squeeze(log_d, squeeze_dims=[1])\n",
        "\n",
        "def data_network_xx(x,x_hat, n_layers=1, n_hidden=128, activation_fn=None, reuse=False):\n",
        "    \"\"\"Approximate x log data density.\"\"\"\n",
        "    # pdb.set_trace()\n",
        "    h = tf.concat([x,x_hat], 1)\n",
        "    with tf.variable_scope('discriminator_xx', reuse=reuse):\n",
        "        h = slim.stack(h, slim.fully_connected, [n_hidden] * n_layers, activation_fn=tf.nn.relu)\n",
        "        log_d = slim.fully_connected(h, 1, activation_fn=activation_fn)\n",
        "    return tf.squeeze(log_d, squeeze_dims=[1])\n",
        "\n",
        "def data_network_zz(z, z_prime, n_layers=1, n_hidden=128, activation_fn=None, reuse=False):\n",
        "    \"\"\"Approximate x log data density.\"\"\"\n",
        "    # pdb.set_trace()\n",
        "    h = tf.concat([z, z_prime], 1)\n",
        "    with tf.variable_scope('discriminator_zz', reuse=reuse):\n",
        "        h = slim.stack(h, slim.fully_connected, [n_hidden] * n_layers, activation_fn=tf.nn.relu)\n",
        "        log_d = slim.fully_connected(h, 1, activation_fn=activation_fn)\n",
        "    return tf.squeeze(log_d, squeeze_dims=[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HQTheh-PJp9"
      },
      "source": [
        "\"\"\" Construct model and training ops \"\"\"\n",
        "tf.reset_default_graph()\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=(None, input_dim))\n",
        "z = tf.placeholder(tf.float32, shape=(None, latent_dim))\n",
        "\n",
        "# decoder and encoder\n",
        "p_x = generative_network(z, input_dim , n_layer_gen, n_hidden_gen, eps_dim)\n",
        "q_z = inference_network(x, latent_dim, n_layer_inf, n_hidden_inf, eps_dim)\n",
        "\n",
        "decoder_logit = data_network_xz(p_x, z, n_layers=n_layer_disc, n_hidden=n_hidden_disc)\n",
        "encoder_logit = graph_replace(decoder_logit, {p_x: x, z:q_z})\n",
        "\n",
        "decoder_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = tf.zeros_like(decoder_logit), logits=decoder_logit)\n",
        "encoder_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = tf.ones_like(encoder_logit), logits=encoder_logit)\n",
        "\n",
        "dis_loss_xz = tf.reduce_mean(encoder_loss) + tf.reduce_mean(decoder_loss)\n",
        "\n",
        "rec_z = inference_network(p_x, latent_dim, n_layer_inf, n_hidden_inf, eps_dim, reuse=True)\n",
        "rec_x = generative_network(q_z, input_dim , n_layer_gen, n_hidden_gen,  eps_dim, reuse=True)\n",
        "\n",
        "x_logit_real = data_network_xx(x, x)\n",
        "x_logit_fake = data_network_xx(x, rec_x, reuse=True)\n",
        "z_logit_real = data_network_zz(z, z)\n",
        "z_logit_fake = data_network_zz(z, rec_z,  reuse=True)\n",
        "\n",
        "x_sigmoid_real = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit_real, labels=tf.ones_like(x_logit_real))\n",
        "x_sigmoid_fake = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit_fake, labels=tf.zeros_like(x_logit_fake))\n",
        "\n",
        "z_sigmoid_real = tf.nn.sigmoid_cross_entropy_with_logits(logits=z_logit_real, labels=tf.ones_like(z_logit_real))\n",
        "z_sigmoid_fake = tf.nn.sigmoid_cross_entropy_with_logits(logits=z_logit_fake, labels=tf.zeros_like(z_logit_fake))\n",
        "\n",
        "x_sigmoid_real2 = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit_real, labels=tf.zeros_like(x_logit_real))\n",
        "x_sigmoid_fake2 = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit_fake, labels=tf.ones_like(x_logit_fake))\n",
        "\n",
        "z_sigmoid_real2 = tf.nn.sigmoid_cross_entropy_with_logits(logits=z_logit_real, labels=tf.zeros_like(z_logit_real))\n",
        "z_sigmoid_fake2 = tf.nn.sigmoid_cross_entropy_with_logits(logits=z_logit_fake, labels=tf.ones_like(z_logit_fake))\n",
        "\n",
        "\n",
        "dis_loss_x = tf.reduce_mean(x_sigmoid_real + x_sigmoid_fake)\n",
        "dis_loss_z = tf.reduce_mean(z_sigmoid_real + z_sigmoid_fake)\n",
        "disc_loss = dis_loss_xz + dis_loss_x + dis_loss_z\n",
        "\n",
        "cost_x = tf.reduce_mean(x_sigmoid_real2 + x_sigmoid_fake2) # + tf.reduce_mean(tf.pow(x_feature_real - x_feature_fake, 2))\n",
        "cost_z = tf.reduce_mean(z_sigmoid_real2 + z_sigmoid_fake2) # + tf.reduce_mean(tf.pow(z_feature_real - z_feature_fake, 2))\n",
        "\n",
        "\n",
        "decoder_loss2 = tf.nn.sigmoid_cross_entropy_with_logits(labels = tf.ones_like(decoder_logit), logits=decoder_logit)\n",
        "encoder_loss2 = tf.nn.sigmoid_cross_entropy_with_logits(labels = tf.zeros_like(encoder_logit), logits=encoder_logit)\n",
        "\n",
        "gen_loss_xz = tf.reduce_mean(decoder_loss2)  + tf.reduce_mean(encoder_loss2)\n",
        "\n",
        "gen_loss = gen_loss_xz + cost_x  + cost_z\n",
        "\n",
        "qvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"inference\")\n",
        "pvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"generative\")\n",
        "dvars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator\")\n",
        "dvars_xx = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator_xx\")\n",
        "dvars_zz = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"discriminator_zz\")\n",
        "\n",
        "opt = tf.train.AdamOptimizer(1e-3, beta1=0.5)\n",
        "train_gen_op =  opt.minimize(gen_loss, var_list=qvars + pvars)\n",
        "train_disc_op = opt.minimize(disc_loss, var_list=dvars + dvars_xx)\n",
        "\n",
        "mahalanobis_dis_z = tf.norm(q_z, ord=2, axis=1, keep_dims=False, name='z_scores')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x-EviEMPJp9"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MF5K99uvPJp9"
      },
      "source": [
        "\"\"\" training \"\"\"\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.1\n",
        "sess = tf.Session(config=config)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "FG = []\n",
        "FD = []\n",
        "\n",
        "for epoch in tqdm( range(n_epoch), total=n_epoch):\n",
        "    X_dataset= shuffle(X_dataset)\n",
        "    Z_dataset= shuffle(Z_dataset)\n",
        "    i = 0\n",
        "    for xmb, zmb in iter_data(X_dataset, Z_dataset, size=batch_size):\n",
        "        i = i + 1\n",
        "        for _ in range(1):\n",
        "            f_d, _ = sess.run([disc_loss, train_disc_op], feed_dict={x: xmb, z:zmb})\n",
        "        for _ in range(5):\n",
        "            f_g, _ = sess.run([[gen_loss, gen_loss_xz, cost_x], train_gen_op], feed_dict={x: xmb, z:zmb})\n",
        "\n",
        "        FG.append(f_g)\n",
        "        FD.append(f_d)\n",
        "\n",
        "    print(\"epoch %d iter %d: discloss %f genloss %f adv_x %f recons_x %f \" % (epoch, i, f_d, f_g[0], f_g[1], f_g[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EbZbOeLPJp-"
      },
      "source": [
        "## Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "FpsyaR9lPJp-"
      },
      "source": [
        "datasetX_test = four_five_gaussians(p1=0)\n",
        "save_path_x = result_dir + 'X_4gmm_data_test.png'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "1b24Jn_PPJp-"
      },
      "source": [
        "# create X dataset\n",
        "\n",
        "X_np_data_test = datasetX_test.data['samples']\n",
        "X_targets_test = datasetX_test.data['label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2DsLao9PJp-"
      },
      "source": [
        "\n",
        "X_np_outliers = np.random.uniform(low=X_np_data_test.min(), high=X_np_data_test.max(), size=(N_NOISY,2))#outlier testing size 20\\n\"\n",
        "X_outliers_target = [15]* X_np_outliers.shape[0]\n",
        "X_np_data_test = np.concatenate([X_np_data_test, X_np_outliers], axis=0)\n",
        "X_targets_test = np.concatenate([X_targets_test, X_outliers_target], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTKrzRUaPJp_"
      },
      "source": [
        "Selecting inliers and outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "5QdRk_SRPJp_"
      },
      "source": [
        "idx_inliers = X_targets_test <4 \n",
        "idx_outliers = np.logical_not(idx_inliers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP4SuHs8PJp_"
      },
      "source": [
        "Plotting final (cleaned) test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl7WUXhoPJp_"
      },
      "source": [
        "plt.clf()\n",
        "fig_mx, ax = plt.subplots(nrows=1, ncols=1, figsize=(4.5, 4.5))\n",
        "ax.scatter(X_np_data_test[idx_inliers, 0], X_np_data_test[idx_inliers, 1], c=cm.tab20(X_targets_test[idx_inliers].astype(float)/15.0),\n",
        "           edgecolor='none', alpha=0.5, label=\"Normal samples\")\n",
        "\n",
        "ax.scatter(X_np_data_test[idx_outliers, 0], X_np_data_test[idx_outliers, 1], c=cm.tab20(X_targets_test[idx_outliers].astype(float)/15),\n",
        "       edgecolor='none', alpha=0.5, label=\"Anomalous samples\", marker='x')\n",
        "\n",
        "ax.set_xlabel('$x_1$'); ax.set_ylabel('$x_2$')\n",
        "ax.axis('on')\n",
        "ax.set_title(\"Test dataset\")\n",
        "ax.legend()\n",
        "plt.savefig(save_path_x, transparent=True, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhey8VRSPJp_"
      },
      "source": [
        "#### Z Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG5BL7rGPJqA"
      },
      "source": [
        "# create Z dataset\n",
        "\n",
        "datasetZ_test = sample_GMM(X_np_data_test.shape[0], means_z, variances_z, priors_z, sources=('features', ))\n",
        "save_path = result_dir + 'Z_gmm_data_test.png'\n",
        "# plot_GMM(dataset, save_path)\n",
        "\n",
        "Z_np_data_test = datasetZ_test.data['samples']\n",
        "Z_targets_test = datasetZ_test.data['label']\n",
        "\n",
        "fig_mx, ax = plt.subplots(nrows=1, ncols=1, figsize=(4.5, 4.5))\n",
        "ax.scatter(Z_np_data_test[:, 0], Z_np_data_test[:, 1],\n",
        "           edgecolor='none', alpha=0.5)\n",
        "ax.set_xlim(-3, 3); ax.set_ylim(-3.5, 3.5)\n",
        "ax.set_xlabel('$z_1$'); ax.set_ylabel('$z_2$')\n",
        "ax.axis('on')\n",
        "plt.savefig(save_path, transparent=True, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eocmAMCTPJqA"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyJ1mqV2PJqA"
      },
      "source": [
        "feed_dict = {x: X_np_data_test, z:Z_np_data_test}\n",
        "imz = sess.run(q_z, feed_dict=feed_dict)\n",
        "rmz = sess.run(rec_z, feed_dict=feed_dict)\n",
        "imx = sess.run(p_x, feed_dict=feed_dict)\n",
        "rmx = sess.run(rec_x, feed_dict=feed_dict)\n",
        "\n",
        "score_dz = sess.run(mahalanobis_dis_z, feed_dict=feed_dict)\n",
        "score_dxx = sess.run(x_sigmoid_real + x_sigmoid_fake, feed_dict=feed_dict)\n",
        "\n",
        "score = score_dz #+ score_dxx\n",
        "score = np.linalg.norm(imz, ord=2, axis=1, keepdims=False)\n",
        "print(score)\n",
        "def zmar(imz):\n",
        "    return np.linalg.norm(imz, ord=2, axis=1, keepdims=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TABYc_b_bfA"
      },
      "source": [
        "kernel estimation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az1GEjkk_dE_"
      },
      "source": [
        "from sklearn.neighbors import KernelDensity\n",
        "import numpy as np\n",
        "feed_dict_train = {x: X_dataset, z:Z_dataset}\n",
        "imz_train = sess.run(q_z, feed_dict=feed_dict_train)\n",
        "kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(imz_train)\n",
        "score_kernel=kde.score_samples(imz)\n",
        "print(score_kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKubLhzFDS8H"
      },
      "source": [
        "hand written digit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8HZJr2KDQoQ"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.neighbors import KernelDensity\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# load the data\n",
        "digits = load_digits()\n",
        "\n",
        "# project the 64-dimensional data to a lower dimension\n",
        "pca = PCA(n_components=15, whiten=False)\n",
        "data = pca.fit_transform(digits.data)\n",
        "\n",
        "# use grid search cross-validation to optimize the bandwidth\n",
        "params = {\"bandwidth\": np.logspace(-1, 1, 20)}\n",
        "grid = GridSearchCV(KernelDensity(), params)\n",
        "grid.fit(data)\n",
        "\n",
        "print(\"best bandwidth: {0}\".format(grid.best_estimator_.bandwidth))\n",
        "\n",
        "# use the best estimator to compute the kernel density estimate\n",
        "kde = grid.best_estimator_\n",
        "\n",
        "# sample 44 new points from the data\n",
        "new_data = kde.sample(44, random_state=0)\n",
        "new_data = pca.inverse_transform(new_data)\n",
        "\n",
        "# turn data into a 4x11 grid\n",
        "new_data = new_data.reshape((4, 11, -1))\n",
        "real_data = digits.data[:44].reshape((4, 11, -1))\n",
        "\n",
        "# plot real digits and resampled digits\n",
        "fig, ax = plt.subplots(9, 11, subplot_kw=dict(xticks=[], yticks=[]))\n",
        "for j in range(11):\n",
        "    ax[4, j].set_visible(False)\n",
        "    for i in range(4):\n",
        "        im = ax[i, j].imshow(\n",
        "            real_data[i, j].reshape((8, 8)), cmap=plt.cm.binary, interpolation=\"nearest\"\n",
        "        )\n",
        "        im.set_clim(0, 16)\n",
        "        im = ax[i + 5, j].imshow(\n",
        "            new_data[i, j].reshape((8, 8)), cmap=plt.cm.binary, interpolation=\"nearest\"\n",
        "        )\n",
        "        im.set_clim(0, 16)\n",
        "\n",
        "ax[0, 5].set_title(\"Selection from the input data\")\n",
        "ax[5, 5].set_title('\"New\" digits drawn from the kernel density model')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bgb-TToPJqA"
      },
      "source": [
        "## inferred marginal z\n",
        "fig_mz, ax = plt.subplots(nrows=1, ncols=1, figsize=(4.5, 4.5))\n",
        "ll = X_targets_test\n",
        "ax.scatter(imz[:, 0], imz[:, 1], c=cm.tab20(ll.astype(float)/input_dim/3.0),\n",
        "        edgecolor='none', alpha=0.5)\n",
        "#ax.set_xlim(-3, 3); ax.set_ylim(-3.5, 3.5)\n",
        "ax.set_xlabel('$z_1$'); ax.set_ylabel('$z_2$')\n",
        "ax.axis('on')\n",
        "ax.set_title(\"Inferred marginal z\")\n",
        "plt.savefig(result_dir + 'inferred_{}_mz.png'.format(DATASET), transparent=True, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOKPgKyuPJqB"
      },
      "source": [
        "import matplotlib as mpl\n",
        "## discriminator z choice on latent - v3\n",
        "fig_mz, ax = plt.subplots(nrows=1, ncols=1, figsize=(6.5, 5))\n",
        "\n",
        "colors = score_kernel#zmar(score)\n",
        "norm = mpl.colors.Normalize(vmin=min(colors), vmax=max(colors))\n",
        "\n",
        "im = ax.scatter(imz[idx_inliers, 0], imz[idx_inliers, 1], c=colors[idx_inliers],\n",
        "        edgecolor='none', alpha=0.5, label=\"Normal samples\", marker='o', norm=norm)\n",
        "\n",
        "im = ax.scatter(imz[idx_outliers, 0], imz[idx_outliers, 1], c=colors[idx_outliers],\n",
        "        edgecolor='none', alpha=0.5, label=\"Anomalous samples\", marker='x', norm=norm)\n",
        "#ax.set_xlim(-3, 3); ax.set_ylim(-3.5, 3.5)\n",
        "ax.set_xlabel('$z_1$'); ax.set_ylabel('$z_2$')\n",
        "fig_mz.colorbar(im)\n",
        "ax.axis('on')\n",
        "plt.axis('equal')\n",
        "ax.set_title(\"Discriminator output in latent space\")\n",
        "plt.savefig(result_dir + 'inferred_{}_mz_disc.png'.format(DATASET), transparent=True, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAygviacPJqB"
      },
      "source": [
        "## discriminator z choice on latent - v2\n",
        "fig_mz, ax = plt.subplots(nrows=1, ncols=1, figsize=(4.5, 4.5))\n",
        "\n",
        "ax.scatter(imz[idx_inliers][:, 0], imz[idx_inliers][:, 1], c='grey',\n",
        "        edgecolor='none', alpha=0.5)\n",
        "ax.scatter(imz[idx_outliers][:, 0], imz[idx_outliers][:, 1], c='r',\n",
        "        edgecolor='none', alpha=0.5, marker='x')\n",
        "#ax.set_xlim(-3, 3); ax.set_ylim(-3.5, 3.5)\n",
        "ax.set_xlabel('$z_1$'); ax.set_ylabel('$z_2$')\n",
        "ax.axis('on')\n",
        "plt.savefig(result_dir + 'inferred_{}_mz.png'.format(DATASET), transparent=True, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IEcvMwlPJqC"
      },
      "source": [
        "##  reconstructed z\n",
        "fig_pz, ax = plt.subplots(nrows=1, ncols=1, figsize=(4.5, 4.5))\n",
        "\n",
        "ax.scatter(rmz[:, 0], rmz[:, 1],\n",
        "           edgecolor='none', alpha=0.5)\n",
        "#ax.set_xlim(-3, 3); ax.set_ylim(-3.5, 3.5)\n",
        "ax.set_xlabel('$z_1$'); ax.set_ylabel('$z_2$')\n",
        "ax.axis('on')\n",
        "ax.set_title('Reconstructed latent space')\n",
        "plt.savefig(result_dir + 'reconstruct_{}_mz.png'.format(DATASET), transparent=True, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCXCt3ncPJqC"
      },
      "source": [
        "## inferred marginal x\n",
        "fig_pz, ax = plt.subplots(nrows=1, ncols=1, figsize=(4.5, 4.5))\n",
        "ax.scatter(imx[:, 0], imx[:, 1],\n",
        "        edgecolor='none', alpha=0.5)\n",
        "#ax.set_xlim(-3, 3); ax.set_ylim(-3.5, 3.5)\n",
        "ax.set_xlabel('$x_1$'); ax.set_ylabel('$x_2$')\n",
        "ax.axis('on')\n",
        "ax.set_title(\"Inferred marginal x\")\n",
        "plt.savefig(result_dir + 'inferred_{}_mx.png'.format(DATASET), transparent=True, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhzH_q3JPJqC"
      },
      "source": [
        "##  reconstructed x\n",
        "fig_mx, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4.5))\n",
        "ax[0].scatter(X_np_data_test[idx_inliers, 0], X_np_data_test[idx_inliers, 1], c=cm.tab20(X_targets_test[idx_inliers].astype(float)/input_dim/3.0),\n",
        "           edgecolor='none', alpha=0.5, label=\"Normal samples\")\n",
        "ax[0].scatter(X_np_data_test[idx_outliers, 0], X_np_data_test[idx_outliers, 1], c=cm.tab20(X_targets_test[idx_outliers].astype(float)/input_dim/3.0),\n",
        "           edgecolor='none', alpha=0.5, label=\"Anomalous samples\", marker='x')\n",
        "ax[0].set_xlabel('$x_1$'); ax[0].set_ylabel('$x_2$')\n",
        "ax[0].axis('on')\n",
        "ax[0].set_xlim(-3, 3); ax[0].set_ylim(-3, 3)\n",
        "ax[0].legend()\n",
        "ax[0].set_title(\"Input x\")\n",
        "\n",
        "ax[1].scatter(rmx[idx_inliers, 0], rmx[idx_inliers, 1], c=cm.tab20(X_targets_test[idx_inliers].astype(float)/input_dim/3.0),\n",
        "           edgecolor='none', alpha=0.5, label=\"Normal samples\")\n",
        "ax[1].scatter(rmx[idx_outliers, 0], rmx[idx_outliers, 1], c=cm.tab20(X_targets_test[idx_outliers].astype(float)/input_dim/3.0),\n",
        "           edgecolor='none', alpha=0.5, label=\"Anomalous samples\", marker='x')\n",
        "ax[1].set_xlim(-3, 3); ax[1].set_ylim(-3, 3)\n",
        "ax[1].set_xlabel('$x_1$'); ax[1].set_ylabel('$x_2$')\n",
        "ax[1].axis('on')\n",
        "ax[1].legend()\n",
        "ax[1].set_title(\"Reconstructed x\")\n",
        "plt.savefig(result_dir + 'reconstruct_{}_mx.png'.format(DATASET), transparent=True, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBFQZ62cPJqC"
      },
      "source": [
        "## learning curves\n",
        "fig_curve, ax = plt.subplots(nrows=1, ncols=1, figsize=(4.5, 4.5))\n",
        "ax.plot(FD, label=\"Discriminator\")\n",
        "ax.plot(np.array(FG)[:,0], label=\"Generator\")\n",
        "ax.plot(np.array(FG)[:,1], label=\"Reconstruction x\")\n",
        "ax.plot(np.array(FG)[:,2], label=\"Reconstruction Z\")\n",
        "plt.xlabel('Iteration')\n",
        "plt.xlabel('Loss')\n",
        "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "ax.axis('on')\n",
        "ax.set_title(\"Learning curves {}\".format(DATASET))\n",
        "plt.savefig(result_dir + 'learning_curves_{}.png'.format(DATASET), bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}